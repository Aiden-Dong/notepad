createdAt: "2019-12-23T09:31:31.331Z"
updatedAt: "2019-12-24T11:34:49.679Z"
type: "MARKDOWN_NOTE"
folder: "d218c9600accb5c277d2"
title: "Spark 资源篇 | Spark On Yarn/k8s"
tags: []
content: '''
  ### Spark 资源篇 | Spark On Yarn/k8s
  
  ---
  
  本文通过阅读 Spark deploy 相关部分的代码，了解学习 spark 分布式资源调度的架构实现以及原理和细节。
  
  参考的 spark 版本为 2.3.1.
  
  在spark源码中，主要实现了三种分布式环境下的资源申请与作业调度方案，他们分别是 : 
  
  - [yarn](https://github.com/Aiden-Dong/spark-2.3.1/tree/master/resource-managers/yarn) : https://github.com/Aiden-Dong/spark-2.3.1/tree/master/resource-managers/yarn
  - [kubernetes](https://github.com/Aiden-Dong/spark-2.3.1/tree/master/resource-managers/kubernetes) : https://github.com/Aiden-Dong/spark-2.3.1/tree/master/resource-managers/kubernetes
  - [mesos](https://github.com/Aiden-Dong/spark-2.3.1/tree/master/resource-managers/mesos) : https://github.com/Aiden-Dong/spark-2.3.1/tree/master/resource-managers/mesos
  
  
  我们下面主要介绍下 `yarn`, `kubernetes` 环境下的资源调度实现。
  
  
  ---
  
  ### SchedulerBackend : 资源调度的本质
  
  分布式情况下的资源调度框架是相同的，服务的交互依赖于 RPC 的相关模块， 有关 RPC 可以阅读 : https://aiden-dong.github.io/2019/12/12/spark%E6%BA%90%E7%A0%81%E4%B9%8BRPC%E7%AF%87/ 部分。
  
  在分布式下面， driver 端的类实现为 `CoarseGrainedSchedulerBackend` 的相关派生类。 executor 端的类实现为 `CoarseGrainedExecutorBackend`.
  
  ![image.png](https://github.com/Aiden-Dong/Aiden-Dong.github.io/raw/master/assets/spark_deploy_1_1.png)
  
  `CoarseGrainedSchedulerBackend` 是 `SchedulerBackend` 的派生类， 其主要作用是**检查当前是否有可用的资源，并负责将需要调度的作业提交到相应的executor中**.
  
  `CoarseGrainedSchedulerBackend` 内部使用 `DriverEndpoint` 与 `CoarseGrainedExecutorBackend` 通信， `DriverEndpoint` 与 `CoarseGrainedExecutorBackend` 本质上是一个 `RpcEndpoint`。两者通过 Netty 实现通信。
  
  ![image.png](https://github.com/Aiden-Dong/Aiden-Dong.github.io/raw/master/assets/spark_deploy_1_3.png)
  
  driver 端服务`CoarseGrainedSchedulerBackend`首先被启动， 然后在根据需求executor数量，在启动相应数量的 `CoarseGrainedExecutorBackend`.  `CoarseGrainedExecutorBackend` 在启动时根据 driver `DriverEndpoint` 的地址建立链接。 发起注册申请，将自己注册到 `CoarseGrainedSchedulerBackend` 中。
  
  ![image.png](https://github.com/Aiden-Dong/Aiden-Dong.github.io/raw/master/assets/spark_deploy_1_2.png)
  
  driver 端的启动建立逻辑
  
  ```
  val ENDPOINT_NAME = "CoarseGrainedScheduler"
  
  override def start() {
    。。。
    // TODO (prashant) send conf instead of properties
    driverEndpoint = createDriverEndpointRef(properties)
  }
    
  protected def createDriverEndpointRef(
      properties: ArrayBuffer[(String, String)]): RpcEndpointRef = {
    rpcEnv.setupEndpoint(ENDPOINT_NAME, createDriverEndpoint(properties))
  }
  
  class DriverEndpoint(override val rpcEnv: RpcEnv, sparkProperties: Seq[(String, String)])
    extends ThreadSafeRpcEndpoint with Logging {
  
    protected val executorsPendingLossReason = new HashSet[String]
  
    protected val addressToExecutorId = new HashMap[RpcAddress, String]
  
    override def onStart() {
      // 启动时初始化动作
    }
  
    
    /**
     * 无响应 RPC 请求
     */
    override def receive: PartialFunction[Any, Unit] = {
    
      case StatusUpdate(executorId, taskId, state, data) =>
        // task 状态更新消息
        scheduler.statusUpdate(taskId, state, data.value)
        。。。
  
      case ReviveOffers =>
        makeOffers()
  
      case KillTask(taskId, executorId, interruptThread, reason) =>
        // task kill 信息
        。。。
  
      case KillExecutorsOnHost(host) =>
        // executor kill 信息
  
      case UpdateDelegationTokens(newDelegationTokens) =>
      
  
      case RemoveExecutor(executorId, reason) =>
        // executor remove 信息
    }
  
    /**
     * 响应式 RPC 请求信息
     */
    override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {
  
      case RegisterExecutor(executorId, executorRef, hostname, cores, logUrls) =>
        // Executor 注册消息
        if (executorDataMap.contains(executorId)) {
         // RegisterExecutorFailed
        } else if (scheduler.nodeBlacklist != null &&scheduler.nodeBlacklist.contains(hostname)) {
          // RegisterExecutorFailed
        } else {
          ...
          val data = new ExecutorData(executorRef, executorAddress, hostname,cores, cores, logUrls)
          ...
          executorRef.send(RegisteredExecutor)
          ...
          makeOffers()
        }
  
      case StopDriver =>
        context.reply(true)
        stop()
  
      case StopExecutors =>
        ...
        for ((_, executorData) <- executorDataMap) {
          executorData.executorEndpoint.send(StopExecutor)
        }
        ...
  
      case RemoveWorker(workerId, host, message) =>
        ...
        removeWorker(workerId, host, message)
        ...
  
      case RetrieveSparkAppConfig =>
        ...
    }
  
    // 更新资源提交作业
    private def makeOffers() {
      // Make sure no executor is killed while some task is launching on it
      // 提交给 TaskScheduler 来分配任务
      val taskDescs = CoarseGrainedSchedulerBackend.this.synchronized {
        // Filter out executors under killing
        // 过滤掉正在被杀死的executor
        val activeExecutors = executorDataMap.filterKeys(executorIsAlive)
        // 把所有可用的executor封装成资源对象
        val workOffers = activeExecutors.map {
          case (id, executorData) =>
            new WorkerOffer(id, executorData.executorHost, executorData.freeCores)
        }.toIndexedSeq
  
        // 把这些可用的资源交给TaskSchedulerImpl进行调度
        // TaskSchedulerImpl 会综合考虑任务本地性，黑名单，调度池的调度顺序等因素，返回TaskDescription集合
        // TaskDescription对象是对一个Task的完整描述，
        // 包括序列化的任务数据，任务在哪个executor上运行，依赖文件和jar包等信息
        scheduler.resourceOffers(workOffers)
      }
      if (!taskDescs.isEmpty) {
        // 首先还得接着回到 DriverEndpoint.makeOffers 方法，makeOffers方法中通过调用 TaskSchedulerImpl.resourceOffers 方法切入 TaskSchedulerImpl ，
        // 然后就是 TaskSchedulerImpl 在做任务分配的工作，最终 TaskSchedulerImpl 将分配好的任务以TaskDescription的封装形式返回给DriverEndpoint（DriverEndpoint是调度后端的一个内部类）.
        // 然后紧接着调用 DriverEndpoint.launchTasks 方法将这些任务传给相应的executor执行。
        launchTasks(taskDescs)
      }
    }
  
  
    private def launchTasks(tasks: Seq[Seq[TaskDescription]]) {
      for (task <- tasks.flatten) {
        val serializedTask = TaskDescription.encode(task)
        ...
        // 维护cpu资源信息
        val executorData = executorDataMap(task.executorId)
        executorData.freeCores -= scheduler.CPUS_PER_TASK
        // 通过rpc发送任务到指定的executor上
        executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))
      }
    }
    
    private def removeExecutor(executorId: String, reason: ExecutorLossReason): Unit = {
      。。。
    }
  
    private def removeWorker(workerId: String, host: String, message: String): Unit = {
      。。。
    }
  
    protected def disableExecutor(executorId: String): Boolean = {
      。。。
    }
  }
  
  ```
  
  executor 端的启动建立逻辑
  
  ```
  def main(args: Array[String]) {
    。。。
    run(driverUrl, executorId, hostname, cores, appId, workerUrl, userClassPath)
    System.exit(0)
  }
  
  
  private def run(
      driverUrl: String,
      executorId: String,
      hostname: String,
      cores: Int,
      appId: String,
      workerUrl: Option[String],
      userClassPath: Seq[URL]) {
      。。。
      // 建立 Netty RPC 
      val fetcher = RpcEnv.create("driverPropsFetcher",hostname, -1, executorConf, new SecurityManager(executorConf), clientMode = true)
      // 建立与 driver 的链接
      val driver = fetcher.setupEndpointRefByURI(driverUrl)
      // 获取配置
      val cfg = driver.askSync[SparkAppConfig](RetrieveSparkAppConfig)
      val props = cfg.sparkProperties ++ Seq[(String, String)](("spark.app.id", appId))
      fetcher.shutdown()
  
      // Create SparkEnv using properties we fetched from the driver.
      val driverConf = new SparkConf()
      for ((key, value) <- props) {
        // this is required for SSL in standalone mode
        if (SparkConf.isExecutorStartupConf(key)) {
          driverConf.setIfMissing(key, value)
        } else {
          driverConf.set(key, value)
        }
      }
      。。。
  
      val env = SparkEnv.createExecutorEnv(driverConf, executorId, hostname, cores, cfg.ioEncryptionKey, isLocal = false)
      env.rpcEnv.setupEndpoint("Executor", new CoarseGrainedExecutorBackend(env.rpcEnv, driverUrl, executorId, hostname, cores, userClassPath, env))
      。。。
    }
  }
  
  
  private[spark] class CoarseGrainedExecutorBackend(
      override val rpcEnv: RpcEnv,
      driverUrl: String,
      executorId: String,
      hostname: String,
      cores: Int,
      userClassPath: Seq[URL],
      env: SparkEnv)
    extends ThreadSafeRpcEndpoint with ExecutorBackend with Logging {
  
    private[this] val stopping = new AtomicBoolean(false)
    var executor: Executor = null
    @volatile var driver: Option[RpcEndpointRef] = None
  
    private[this] val ser: SerializerInstance = env.closureSerializer.newInstance()
  
    override def onStart() {
      // 初始化
      rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap { ref =>
        // 注册到 driver
        driver = Some(ref)
        ref.ask[Boolean](RegisterExecutor(executorId, self, hostname, cores, extractLogUrls))
      }(ThreadUtils.sameThread).onComplete {
       ...
      }(ThreadUtils.sameThread)
    }
  
  
    override def receive: PartialFunction[Any, Unit] = {
      case RegisteredExecutor =>
        // 注册成功 
        executor = new Executor(executorId, hostname, env, userClassPath, isLocal = false)
  
      case RegisterExecutorFailed(message) =>
        // 注册失败
  
      case LaunchTask(data) =>
        // 运行作业
        executor.launchTask(this, taskDesc)
  
      case KillTask(taskId, _, interruptThread, reason) =>
        if (executor == null) {
          exitExecutor(1, "Received KillTask command but executor was null")
        } else {
          executor.killTask(taskId, interruptThread, reason)
        }
  
      case StopExecutor =>
        。。。
  
      case Shutdown =>
        。。。
  
      case UpdateDelegationTokens(tokenBytes) =>
        。。。
    }
  }
  ```
  
  所以每当executor 启动后， 会根据 driver 的信息自动向 driver 的`DriverEndpoint`发起注册请求。这时候每个 executor 的相关信息被维护到 `CoarseGrainedSchedulerBackend` 的`executorDataMap` 变量中。
  
  ```
  private val executorDataMap = new HashMap[String, ExecutorData]
  
  private[cluster] class ExecutorData(
     val executorEndpoint: RpcEndpointRef,
     val executorAddress: RpcAddress,
     override val executorHost: String,
     var freeCores: Int,
     override val totalCores: Int,
     override val logUrlMap: Map[String, String]
  ) extends ExecutorInfo(executorHost, totalCores, logUrlMap)
  ```
  
  当driver端接受到来自`TaskScheduler`或者其他地方的资源更新需求时， 检查自己的executor剩余情况并将资源交给`TaskScheduler`去调度作业.
  完成后，将调度作业信息通过`makeOffers`发送给指定的executor去执行。 (`TaskScheduler` 过程可以参见 https://aiden-dong.github.io/2019/12/09/spark%E6%BA%90%E7%A0%81%E4%B9%8B%E8%B0%83%E5%BA%A6%E7%AF%87/)
  
  ---
  
  ### Spark On Yarn
  
  spark on yarn 环境下允许用户指定 `--deploy-mode` 为 `cluster` or `client`. 即将 driver 端放在本地执行或者在 **Yarn Application Master** 上。
  其他地方都大致相同。
  
  #### cluster mode
  
  ![image.png](https://github.com/Aiden-Dong/Aiden-Dong.github.io/raw/master/assets/spark_deploy_1_4.png)
  
  cluster 模式 driver 在ApplicationMaster中执行。 程序首先加载`org.apache.spark.deploy.yarn.YarnClusterApplication`作为启动类。
  
  ```
  private[deploy] val YARN_CLUSTER_SUBMIT_CLASS = "org.apache.spark.deploy.yarn.YarnClusterApplication"
  
  private def doPrepareSubmitEnvironment(
        args: SparkSubmitArguments,
        conf: Option[HadoopConfiguration] = None)
        : (Seq[String], Seq[String], SparkConf, String) = {
    ...
    val isYarnCluster = clusterManager == YARN && deployMode == CLUSTER
    ...
    if (isYarnCluster) {
        childMainClass = YARN_CLUSTER_SUBMIT_CLASS
        ...
    }
    ...
  }
  
  
  private[spark] class YarnClusterApplication extends SparkApplication {
  
    override def start(args: Array[String], conf: SparkConf): Unit = {
      // SparkSubmit would use yarn cache to distribute files & jars in yarn mode,
      // so remove them from sparkConf here for yarn mode.
      conf.remove("spark.jars")
      conf.remove("spark.files")
  
        new Client(new ClientArguments(args), conf).run()
    }
  
  }
  ```
  
  Yarn 资源的申请 `org.apache.spark.deploy.yarn.Client.submitApplication()` 完成。
  
  ![image.png](https://github.com/Aiden-Dong/Aiden-Dong.github.io/raw/master/assets/spark_deploy_1_6.png)
  
  #### client mode
  
  client 模式 driver 在client中执行。 程序首先加载用户启用主类作为启动类。
  
  ![image.png](https://github.com/Aiden-Dong/Aiden-Dong.github.io/raw/master/assets/spark_deploy_1_5.png)
  
  在 `SparkContext` 初始化过程中， 使用 `YarnClusterManager` 创建`YarnClientSchedulerBackend`, 作为其`SchedulerBackend`.
  在其初始化过程中同样调用 `org.apache.spark.deploy.yarn.Client.submitApplication()` 初始化 yarn 资源
  
  ```
  override def start() {
    。。。
    client = new Client(args, conf)
    bindToYarn(client.submitApplication(), None)
    。。。
  }
  ```
  
  ![image.png](https://github.com/Aiden-Dong/Aiden-Dong.github.io/raw/master/assets/spark_deploy_1_7.png)
  
  #### Yarn ApplicationMaster 初始化
  
  `org.apache.spark.deploy.yarn.Client.submitApplication()` 过程使用 `YarnClient` 去RM申请资源去建立 `ApplicationMaster`。
  
  ```
  def submitApplication(): ApplicationId = {
    var appId: ApplicationId = null
    ...
    yarnClient.init(hadoopConf)
    yarnClient.start()
  
    val newApp = yarnClient.createApplication()                          // 创建一个Application
    val newAppResponse = newApp.getNewApplicationResponse()              // 获取资源情况
    appId = newAppResponse.getApplicationId()
    verifyClusterResources(newAppResponse)                                // 校验内存请求是否合理
  
    val containerContext = createContainerLaunchContext(newAppResponse)   // 设置执行的环境与执行的命令
    val appContext = createApplicationSubmissionContext(newApp, containerContext)  // 设置资源需求
    yarnClient.submitApplication(appContext)                              // 提交作业
    。。。
  
    appId
  
  }
  
  
  private def createContainerLaunchContext(newAppResponse: GetNewApplicationResponse)
    : ContainerLaunchContext = {
    。。。
    val launchEnv = setupLaunchEnv(appStagingDirPath, pySparkArchives)               // 环境
    val localResources = prepareLocalResources(appStagingDirPath, pySparkArchives)   // 依赖
  
    val amContainer = Records.newRecord(classOf[ContainerLaunchContext])
    amContainer.setLocalResources(localResources.asJava)
    amContainer.setEnvironment(launchEnv.asJava)
  
    // 命令
    val javaOpts = ListBuffer[String]()
    。。。
    val userClass =
      if (isClusterMode) {
        Seq("--class", YarnSparkHadoopUtil.escapeForShell(args.userClass))
      } else {
        Nil
      }
    
    val amClass =
      if (isClusterMode) {
        Utils.classForName("org.apache.spark.deploy.yarn.ApplicationMaster").getName    // cluster
      } else {
        Utils.classForName("org.apache.spark.deploy.yarn.ExecutorLauncher").getName     // client
      }
  
    val amArgs =
      Seq(amClass) ++ userClass ++ userJar ++ primaryPyFile ++ primaryRFile ++ userArgs ++
      Seq("--properties-file", buildPath(Environment.PWD.$$(), LOCALIZED_CONF_DIR, SPARK_CONF_FILE))
  
    
    val commands = prefixEnv ++
      Seq(Environment.JAVA_HOME.$$() + "/bin/java", "-server") ++
      javaOpts ++ amArgs ++
      Seq(
        "1>", ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stdout",
        "2>", ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stderr")
  
    // TODO: it would be nicer to just make sure there are no null commands here
    val printableCommands = commands.map(s => if (s == null) "null" else s).toList
    amContainer.setCommands(printableCommands.asJava)
    。。。
    amContainer
  }
  ```
  
  当 client 向RM申请一个 `AppMaster` 后， 如果是 **cluster** 模式，将启用 `org.apache.spark.deploy.yarn.ApplicationMaster`。
  而如果是 **client** 模式， 将启用 `org.apache.spark.deploy.yarn.ExecutorLauncher`。
   
  不过虽然调用类不一样， 但是最终都会转到 `ApplicationMaster`.
  
  ```
  object ExecutorLauncher {
    def main(args: Array[String]): Unit = {
      ApplicationMaster.main(args)
    }
  }
  ```
  在 `ApplicationMaster` 里面会检查提交模式为**client**or**cluster**。
  
  ```
  private def runImpl(): Unit = {
    。。。
    if (isClusterMode) {
      runDriver()
    } else {
      runExecutorLauncher()
    }
   。。。
  }
  ```
  
  对于**cluster**模式， 会首先启用**用户主类driver**，然后在去申请**Container**.
  
  ```
  private def runDriver(): Unit = {
    。。。
    userClassThread = startUserApplication()   // 启动driver
  
    ...
    val driverRef = createSchedulerRef(sc.getConf.get("spark.driver.host"),sc.getConf.get("spark.driver.port"))
    // register Application Master And setup executor
    registerAM(sc.getConf, rpcEnv, driverRef, sc.ui.map(_.webUrl))   // 向 ApplicationMaster 注册服务并申请 continer
   ...
  }
  
  // 以线程方式启动 driver
  private def startUserApplication(): Thread = {
    。。。 
    val mainMethod = userClassLoader.loadClass(args.userClass).getMethod("main", classOf[Array[String]]) // 反射调用主类 main 方法
  
    val userThread = new Thread {
      override def run() {
          。。。
          mainMethod.invoke(null, userArgs.toArray)
          。。。
      }
    }
    userThread.setContextClassLoader(userClassLoader)
    userThread.setName("Driver")
    userThread.start()
    userThread
  }
  ```
  
  对于**client**模式， 由于driver已经在client中运行，所以直接申请**Container**即可。
  
  ```
  private def runExecutorLauncher(): Unit = {
    ...
    rpcEnv = RpcEnv.create("sparkYarnAM", hostname, hostname, -1, sparkConf, securityMgr,amCores, true)
    val driverRef = waitForSparkDriver()
    addAmIpFilter(Some(driverRef))
    registerAM(sparkConf, rpcEnv, driverRef, sparkConf.getOption("spark.driver.appUIAddress"))
    ...
  }
  ```
  
  最终， 在`registerAM`时， ApplicationMaster 申请需要的container 并启动`CoarseGrainedExecutorBackend` 服务。
  
  ```
  private def runAllocatedContainers(containersToUse: ArrayBuffer[Container]): Unit = {
    for (container <- containersToUse) {
     ...
      if (runningExecutors.size() < targetNumExecutors) {
        numExecutorsStarting.incrementAndGet()
        if (launchContainers) {
          launcherPool.execute(new Runnable {
            override def run(): Unit = {
                ...
                new ExecutorRunnable(
                  Some(container),
                  conf,
                  sparkConf,
                  driverUrl,
                  executorId,
                  executorHostname,
                  executorMemory,
                  executorCores,
                  appAttemptId.getApplicationId.toString,
                  securityMgr,
                  localResources
                ).run()
                updateInternalState()
             ...
            }
          })
        } else {
          // For test only
          updateInternalState()
        }
      } else {
        logInfo(("Skip launching executorRunnable as running executors count: %d " +
          "reached target executors count: %d.").format(
          runningExecutors.size, targetNumExecutors))
      }
    }
  }
  ```
  
  
'''
linesHighlighted: []
isStarred: false
isTrashed: false
