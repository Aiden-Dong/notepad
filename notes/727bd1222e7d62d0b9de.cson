type: "MARKDOWN_NOTE"
folder: "ef3e1850a2ffff04209d"
title: "6-Hive StorageHandlers"
content: '''
  ### 6-Hive StorageHandlers
  
  ---
   
   参考自 [https://cwiki.apache.org/confluence/display/Hive/StorageHandlers](https://cwiki.apache.org/confluence/display/Hive/StorageHandlers)
   参考自 [https://cwiki.apache.org/confluence/display/Hive/FilterPushdownDev](https://cwiki.apache.org/confluence/display/Hive/FilterPushdownDev)
   
  Hive存储处理程序支持基于Hadoop和Hive中的现有可扩展性功能构建：
  
  - 输入格式
  - 输出格式
  - 序列化/反序列化库
  
  除了将这些绑定在一起之外，存储处理程序还可以实现新的元数据挂钩接口，允许Hive DDL同时并一致地管理Hive Metastore和其他系统的目录中的对象定义。
  
  在存储处理程序之前，Hive已经有了托管 vs 外部表的概念。托管表是定义主要在Hive的Metastore中管理的表，Hive负责其数据存储的表。
  
  外部表是其定义在某个外部目录中进行管理的表，并且其数据Hive不拥有（即在表被删除时不会被删除）。
  
  存储处理程序引入了本地和非本地表之间的区别:
  
  本地表是Hive知道如何管理和访问没有存储处理程序; 
  非本地表是需要存储处理程序的表。
  
  这两个区别（管理与外部和本地与非本地）是正交的。因此，基表有四种可能性：
  
  ```
  managed native      ： 默认使用CREATE TABLE获得的内容
  external native     : 当没有指定STORED BY子句时，用CREATE EXTERNAL TABLE得到的结果
  managed non-native  : 当指定了STORED BY子句时，用CREATE TABLE得到的结果; Hive将定义存储在其Metastore中，但不会自己创建任何文件; 相反，它调用存储处理程序的请求来创建相应的对象结构
  external non-native : 当指定了STORED BY子句时，你用CREATE EXTERNAL TABLE得到了什么; Hive将定义注册在其Metastore中，并调用存储处理程序来检查它是否与其他系统中的主定义相匹配
  ```
  
  ### DDL
  
  存储处理程序在通过新的`STORED BY`子句创建时与表关联，这是现有`ROW FORMAT`和`STORED AS`子句的替代方法：
  
  ```
  CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name
    [(col_name data_type [COMMENT col_comment], ...)]
    [COMMENT table_comment]
    [PARTITIONED BY (col_name data_type [col_comment], col_name data_type [COMMENT col_comment], ...)]
    [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name, ...)] INTO num_buckets BUCKETS]
    [
     [ROW FORMAT row_format] [STORED AS file_format]
     | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]
    ]
    [LOCATION hdfs_path]
    [AS select_statement]
  ```
  
  当指定STORED BY时，则不能指定row_format（DELIMITED或SERDE）和STORED AS。可选SERDEPROPERTIES可以被指定为STORED BY子句的一部分，并将被传递给存储处理程序提供的serde。
  
  ```
  CREATE TABLE hbase_table_1(key int, value string)
  STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  WITH SERDEPROPERTIES (
  "hbase.columns.mapping" = "cf:string",
  "hbase.table.name" = "hbase_table_0"
  );
  ```
  
  ### 存储处理程序接口
  
  ```
  package org.apache.hadoop.hive.ql.metadata;
   
  import java.util.Map;
   
  import org.apache.hadoop.conf.Configurable;
  import org.apache.hadoop.hive.metastore.HiveMetaHook;
  import org.apache.hadoop.hive.ql.plan.TableDesc;
  import org.apache.hadoop.hive.serde2.SerDe;
  import org.apache.hadoop.mapred.InputFormat;
  import org.apache.hadoop.mapred.OutputFormat;
   
  public interface HiveStorageHandler extends Configurable {
    public Class<? extends InputFormat> getInputFormatClass();
    public Class<? extends OutputFormat> getOutputFormatClass();
    public Class<? extends SerDe> getSerDeClass();
    public HiveMetaHook getMetaHook();
    public void configureTableJobProperties(
      TableDesc tableDesc,
      Map<String, String> jobProperties);
  }
  ```
  
  这`HiveMetaHook`是可选的，并在下一节中介绍。如果`getMetaHook`返回非null，则将返回的对象的方法作为零部件修改操作的一部分进行调用。
  
  该`configureTableJobProperties`方法被称为Hadoop执行任务的一部分。存储处理程序负责检查表定义并在`jobProperties`上设置相应的属性。
  
  在执行时，只有这些jobProperties可用于输入格式，输出格式和serde。
  
  ### HiveMetaHook接口
  
  ```
  package org.apache.hadoop.hive.metastore;
   
  import org.apache.hadoop.hive.metastore.api.MetaException;
  import org.apache.hadoop.hive.metastore.api.Partition;
  import org.apache.hadoop.hive.metastore.api.Table;
   
  public interface HiveMetaHook {
    public void preCreateTable(Table table)
      throws MetaException;
    public void rollbackCreateTable(Table table)
      throws MetaException;
    public void commitCreateTable(Table table)
      throws MetaException;
    public void preDropTable(Table table)
      throws MetaException;
    public void rollbackDropTable(Table table)
      throws MetaException;
    public void commitDropTable(Table table, boolean deleteData)
      throws MetaException;
  ```
  
  请注意，不管在Hive配置中是否使用远程Thrift Metastore流程，元数据钩子调用总是从Hive客户端JVM（从不从Thrift Metastore服务器）进行。
  
  这意味着包含存储处理程序类的jar需要在客户端上可用，而不是在thrift服务器上。
  
  另请注意，对于Hive Metastore和存储处理程序，在元数据事务中没有两阶段落实的功能。
  
  因此，在DDL中出现崩溃的小窗口会导致两个系统不同步。
  
  ### FilterPushdownDev
  
  本文解释了我们如何计划在Hive的优化器中添加支持，以将过滤器降低到物理访问方法。
  
  这是一个重要的优化，用于最大限度地减少访问方法扫描和处理的数据量（例如，用于索引键查找），以及减少传递到Hive以供进一步查询评估的数据量。
  
  > 用例:
  
  - 将筛选器推入Hive的内置存储格式，例如RCFile
  - 将过滤器放到存储处理程序中，如HBase处理程序（http://issues.apache.org/jira/browse/HIVE-1226）
  - 一旦将索引框架添加到Hive（http://issues.apache.org/jira/browse/HIVE-417），将筛选器向下推入索引访问计划
  
'''
tags: []
isStarred: false
isTrashed: false
createdAt: "2017-12-29T06:03:41.102Z"
updatedAt: "2017-12-29T09:22:19.710Z"
