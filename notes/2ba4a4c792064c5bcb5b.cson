type: "MARKDOWN_NOTE"
folder: "d218c9600accb5c277d2"
title: "1-spark 介绍"
content: '''
  ### 1-spark 介绍
  
  ---
  
  #### 1. spark 组件:
  
  ![](http://img.my.csdn.net/uploads/201710/14/1507978789_1845.jpg)
  
  Spark四大组件包括Spark Streaming、Spark SQL、Spark MLlib和Spark GraphX。它们的主要应用场景是:
  
  Spark Streaming:
  Spark Streaming基于微批量方式的计算和处理，可以用于处理实时的流数据。它使用DStream，简单来说就是一个弹性分布式数据集（RDD）系列，处理实时数据。
  
  Spark SQL:
  Spark SQL可以通过JDBC API将Spark数据集暴露出去，而且还可以用传统的BI和可视化工具在Spark数据上执行类似SQL的查询。用户还可以用Spark SQL对不同格式的数据（如JSON，Parquet以及数据库等）执行ETL，将其转化，然后暴露给特定的查询。
  
  Spark MLlib:
  MLlib是一个可扩展的Spark机器学习库，由通用的学习算法和工具组成，包括二元分类、线性回归、聚类、协同过滤、梯度下降以及底层优化原语。用于机器学习和统计等场景
  
  Spark GraphX:
  GraphX是用于图计算和并行图计算的新的（alpha）Spark API。通过引入弹性分布式属性图（Resilient Distributed Property Graph），一种顶点和边都带有属性的有向多重图，扩展了Spark RDD。为了支持图计算，GraphX暴露了一个基础操作符集合（如subgraph，joinVertices和aggregateMessages）和一个经过优化的Pregel API变体。此外，GraphX还包括一个持续增长的用于简化图分析任务的图算法和构建器集合。
  
  
  #### 2. spark 集群环境配置
  
  yarn-site.xml
  
  ```
   <property>
      <name>yarn.nodemanager.pmem-check-enabled</name>
      <value>false</value>
    </property>
  
    <property> 
      <name>yarn.nodemanager.vmem-check-enabled</name>
      <value>false</value>
    </property>
  
  ```
  
  spark-env.sh
  
  ```
  export SPARK_DIST_CLASSPATH=$(/usr/local/share/hadoop/bin/hadoop classpath)
  export SPARK_CONF_DIR=/usr/local/share/spark-2.1.0-bin-hadoop2.7/conf
  # 设置运行在Yarn 环境之下
  export HADOOP_CONF_DIR=/usr/local/share/hadoop/etc/hadoop
  export SPARK_MASTER_IP=norma
  export SPARK_WORKER_MEMORY=4g
  export SPARK_WORKER_CORES=1
  export SPARK_EXECUTOR_MEMORY=1g
  export SPARK_DRIVER_MEMORY=1g
  ```
  
  spark-default.conf
  
  ```
  spark.master                     spark://localhost:7077
  spark.eventLog.enabled           true
  spark.eventLog.dir               hdfs://localhost:9000/tmp/spark
  spark.yarn.historyServer.address localhost:19888
  spark.driver.memory              1g
  spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
  ```
  
  #### 2. spark 第一个作业　wordcount:
  
  ```
  object WordCount {
  
    def main(args: Array[String]): Unit = {
      val conf:SparkConf = new SparkConf().setAppName("WordCount")
      val sc:SparkContext = new SparkContext(conf)
  
      val fin = sc.textFile("/data/input/dpkg.log")
  
      val result = fin.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)
  
      result.saveAsTextFile("/data/output/dpkg")
    }
  }
  ```
  
  - spark 环境 : `2.1.0`
  - scala 版本 : `2.11.8`
  - mvn spark 依赖　:
  
  ```
  <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_2.11</artifactId>
      <version>2.1.0</version>
      <scope>provided</scope>
  </dependency>
  ```
  
  - 启动　spark　集群:
  
  ```
  sbin/start-all.sh
  # 启动　Master
  # 启动　Worker
  ```
  
  - 提交作业：
  
  单机作业　: `spark-submit --master spark://norma:7077 --class main.WordCount wordcount-1.0-selfcontained.jar`
  Yarn 作业　: `spark-submit --class main.WordCount --master yarn-cluster wordcount-1.0-selfcontained.jar`
  
  
  
'''
tags: []
isStarred: false
isTrashed: false
createdAt: "2017-10-14T11:01:31.561Z"
updatedAt: "2017-10-31T06:18:49.630Z"
