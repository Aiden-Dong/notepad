type: "MARKDOWN_NOTE"
folder: "d218c9600accb5c277d2"
title: "4-spark shell"
content: '''
  ### 4-spark shell
  
  ---
  
  在 Spark shell 中， 有一个专有的 SparkContext 已经为你创建好。 在变量中叫做 sc 。 你自己创建的 SparkContext 将无法工作。
  可以用 --master 参数来设置 SparkContext 要连接的集群， 用 --jars 来设置需要添加到 classpath 中的 JAR 包， 如果有多个 JAR 包使用逗号分
  割符连接它们。 
  
  ```
  spark-shell --master yarn --deploy-mode client
  ```
  
  运行样例:
  
  ```
  val lines = sc.textFile("data.txt")
  val lineLengths = lines.map(s => s.length)
  val totalLength = lineLengths.reduce((a, b) => a + b)
  ```
  
  第一行是定义来自于外部文件的 RDD。 这个数据集并没有加载到内存或做其他的操作： lines 仅仅是一个指向文件的指针。
  第二行是定义 lineLengths ， 它是 map 转换(transformation)的结果。 同样， lineLengths 由于懒惰模式也没有立即计算。最后，我们执行reduce，它是一个动作(action).
  在这个地方， Spark 把计算分成多个任务(task)， 并且让它们运行在多个机器上。 每台机器都运行自己的 map 部分和本地 reduce 部分。 然后仅仅将结果返回给驱动程序。
  
  如果我们想要再次使用 lineLengths ， 我们可以添加
  
  ```
  lineLengths.persist()
  ```
  
  在 reduce 之前， 它会导致 lineLengths 在第一次计算完成之后保存到内存中。
  
  
'''
tags: []
isStarred: false
isTrashed: true
createdAt: "2017-10-24T10:29:06.978Z"
updatedAt: "2018-06-05T06:43:59.273Z"
