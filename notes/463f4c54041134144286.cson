type: "MARKDOWN_NOTE"
folder: "d218c9600accb5c277d2"
title: "4- spark shell"
content: '''
  ### 4- spark shell
  
  
  
  ---
  
  ```
  val lines = sc.textFile("data.txt")
  val lineLengths = lines.map(s => s.length)
  val totalLength = lineLengths.reduce((a, b) => a + b)
  ```
  
  第一行是定义来自于外部文件的 RDD。 这个数据集并没有加载到内存或做其他的操作： lines 仅仅是一个指向文件的指针。
  第二行是定义 lineLengths ， 它是 map 转换(transformation)的结果。 同样， lineLengths 由于懒惰模式也没有立即计算。最后，我们执行reduce，它是一个动作(action).
  在这个地方， Spark 把计算分成多个任务(task)， 并且让它们运行在多个机器上。 每台机器都运行自己的 map 部分和本地 reduce 部分。 然后仅仅将结果返回给驱动程序。
  
  如果我们想要再次使用 lineLengths ， 我们可以添加
  
  ```
  lineLengths.persist()
  ```
  
  在 reduce 之前， 它会导致 lineLengths 在第一次计算完成之后保存到内存中。
  
  
'''
tags: []
isStarred: false
isTrashed: false
createdAt: "2017-10-24T10:29:06.978Z"
updatedAt: "2017-10-24T10:33:00.655Z"
