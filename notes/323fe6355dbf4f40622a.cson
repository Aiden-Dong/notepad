type: "MARKDOWN_NOTE"
folder: "3d96f15f77c92116ffd3"
title: "6-RADStack：用于交互式分析的开源Lambda架构"
content: '''
  ### 6-RADStack：用于交互式分析的开源Lambda架构
  
  ---
  
  ### 1. 摘要
  
  实时数据分析堆栈，通俗地称为RADStack，是一种开源数据分析堆栈，旨在通过数据提供快速，灵活的查询。 它旨在克服纯批处理系统（表现新事物需要太长时间）或纯实时系统的限制（无法确保没有数据被遗留下来，并且通常无法纠正初始处理后的数据）。 它将无缝地返回最新数据的最佳效果，并结合旧数据的保证正确的结果。 在本文中，我们将介绍RADStack的体系结构，并讨论我们提供交互式分析和灵活数据处理环境的方法来处理各种实际工作负载。
  
  ### 2. 介绍
  
  Hadoop生态系统的快速增长使许多组织能够灵活地处理并获得大量数据的分析能力。 这些分析通常由商业智能或在线分析处理（OLAP）查询生成。 Hadoop已经被证明是一个非常有效的框架，能够提供许多分析，能够解决广泛的分布式计算问题。 然而，就使用Hadoop的广泛例子而言，由于它在处理和返回结果方面的高延迟，所以被嘲笑。 一些常见的数据分析可能需要运行几个小时的MR作业。
  
  
  
  数据分析和数据驱动应用程序在行业中变得越来越重要，使用批处理框架（如Hadoop）遇到的长时间查询时间变得越来越难以忍受。面向用户的应用程序正在取代传统的报告界面作为组织从其数据集中获取价值的首选方法。为了提供与数据应用程序的用户交互式体验，查询必须以毫秒级的顺序完成。由于大多数这些交互围绕数据探索和计算，组织很快意识到，为了支持低延迟查询，专用服务层是必要的。今天，大多数这些服务层是关系数据库管理系统（RDBMS）或NoSQL k/v存储。 RDBMS和NoSQL K/V存储都不是特别设计用于分析，但是这些技术仍然经常被选择为服务层。涉及这些广泛关注技术的解决方案可以在针对分析案例进行定制后才能实现，或者从架构的缺点出发，使它们快速响应查询，以提供交互式，面向用户的应用程序.
  
  
  
  一个理想的数据服务层通常不是一个完整的分析解决方案。在大多数现实世界的案例中，原始数据不能直接存储在服务层。原始数据来自许多缺陷，必须首先在可用之前进行处理（转换或清除）。这个要求的缺点使得加载和处理批量数据变得慢，事件发生后的几个小时之前无法获得对事件的分析。
  
  
  
  
  
  为了解决批量处理框架引起的数据新鲜度的延迟，许多开放源码流处理框架，如Apache Storm，Apache Spark Streaming 和Apache Samza 已经受到欢迎，延迟模型以接近实时的速度摄取和处理事件流。几乎所有流处理器的缺点是它们不一定提供与批处理框架相同的正确性保证。事件可能会在几天之后，事实数据可能需要纠正。当添加或删除新列时，还可能需要重新处理大批数据。
  
  
  
  RADStack是一种开源的端到端解决方案，旨在实现对近实时数据的低延迟分析查询。该解决方案结合了流处理器的低延迟保证以及批处理器的正确性和灵活性保证。它还引入了专门用于交互式分析的服务层。堆栈的主要构建块是 Apache Kafka， Apache Samza，Apache Hadoop 和 Druid，我们发现，技术的组合足以满足各种处理要求和查询负载。堆叠的每一块都是为了做一些特定的事情而设计的。本文将介绍RADStack的细节和设计原则。我们的贡献在于堆栈本身的架构，Druid 作为服务层的引入，以及统一实时和历史工作流程的模型。
  
  
  
  ### 3. 背景
  
  
  
  RADStack的发展是迭代的。 引进了不同的技术来解决新的问题和使用案例。 RADStack的初始概念始于Metamarkets，这是一家专注于广告技术领域的分析公司，并被其他技术公司采用。 在Metamarkets的早期阶段，我们希望建立一个分析仪表板，用户可以随意探索数据。 在线广告空间中发现的数据的一个例子如表1所示。
  
  
  
  表 1：
  
  ![](http://img.my.csdn.net/uploads/201707/24/1500885028_8245.png)
  
  
  
  
  
  表1中显示的数据在OLAP工作流程中相当标准。 数据是具有时间戳的事件流，并且倾向于非常附加。 每个事件由三个组成部分组成：指示事件何时发生的时间戳; 一组指示事件的各种属性的维度; 以及一组关于事件的指标。 我们的目标是通过这些数据快速计算深度和聚合量，以回答诸如“publisher google.com的一周内发生的点击次数”或“上个季度 San Francisco 出现了多少次曝光”？我们希望对任意数量的维度发出查询，几百毫秒就能返回结果。
  
  
  
  除了查询延迟需求外，我们还需要支持多用户。面向用户的应用程序通常面临高度并发的工作负载，良好的应用程序需要为所有用户提供相对一致的性能最后，我们需要我们的后端基础设施来提供高可用性。停机时间是昂贵的，如果系统在软件升级或网络故障时无法使用，许多用户无法等待。
  
  
  
  遵循行业标准做法，我们评估了服务层的各种选项。我们尝试选择针对我们想要查询的类型进行了优化的服务层，最初我们开始对PostgreSQL进行实验。我们使用PostgreSQL的设置遵循使用RDBMS作为数据仓库的常见约定。我们使用星型模式对广告数据建模，我们使用聚合表来提高聚合性能，并覆盖了查询缓存以进一步加快查询速度。
  
  
  
  我们使用PostgreSQL的结果显示在表2中。如果结果被缓存，则查询延迟通常是可接受的，如果我们打入聚合表，或者需要扫描基本事实表，则不可接受。我们应用程序的第一个版本的单页加载，发布了对服务层进行20次并发查询，并且仅需要聚合几个度量，需要几分钟的时间才能完成。这对于交互式应用程序来说太慢了。我们很快意识到一个行存储和关系模型一般是我们试图构建的应用程序的次优选择。
  
  
  
  表 2:
  
  ![](http://img.my.csdn.net/uploads/201707/24/1500885145_1374.png)
  
  
  
  我们接下来评估了HBase ，一个NoSQL K/V存储。使用K/V存储，我们预先计算了我们预期用户将会做的总查询集。这个预先计算的一个例子如图1所示。我们的HBase结果显示在表3中，为50万条记录。这个解决方案的查询是可以接受的，因为我们正在有效地对地图进行O（1）查找。然而，解决方案并不特别灵活;如果有的话不是预先计算的，它是不可查询的，并且预计算时间成为一个很大的痛点。我们试图建立冰山立方体，限制了我们预先计算的空间，但即使这样，预先计算的时间也是不可接受的。
  
  
  
  ![](http://img.my.csdn.net/uploads/201707/24/1500880857_6818.png)
  
  图1：预计算查询按比例缩放。 支持一切可能的组合查询，即使是小数据集也可以产生很多可能的查询
  
  
  
  Druid 的发展是为了解决传统服务层面所遇到的一些问题。Druid 从头开始设计，提供任意数据探索，低延迟聚合和快速数据摄取,Druid也被设计为接受完全非规范化的数据，并远离传统的关系模型。由于大多数原始数据未被非规范化，因此必须先处理它们，然后才能进行查询。在Druid可用之前，必须将多个数据流连接，清理和转换，但这是为了获得为交互式数据应用程序提供支持所需的性能，我们愿意做出的折中。我们向我们的Stack引入了流处理，以便在将原始数据加载到 Druid 之前提供所需的处理。我们的流处理作业范围从简单的数据转换，如id到名称查找，直到复杂的操作，如多流连接。配对 Druid 与流处理器启用了灵活的数据处理和查询，但我们在数据传递中仍然存在问题。我们的数据是从许多不同的地点和来源提供的，每秒达到数百万次的高峰。我们需要一个高吞吐量的消息总线，可以将这些数据用于我们的流处理器去消费。为了简化客户端的数据传输，我们希望将消息总线作为进入集群的事件的单一传送端点。
  
  
  
  如果实时处理是完美的，我们的Stack将是完整的，但开源流处理空间仍然很小。处理作业可能会延长一段时间，事件可能不止一次。这些都是任何生产数据流水线的现实。为了克服这些问题，我们在我们的 Stack 中包括Hadoop，以定期清理实时管道生成的任何数据。我们在分布式文件系统中存储了我们收到的原始事件的副本，并定期在该数据上运行批处理作业。我们的设置的高级架构如图2所示。每个组件都被设计为很好地完成一组特定的操作，并且在功能方面存在隔离。单个组件可以完全失败，而不影响其他组件的服务。
  
  
  
  
  
  ![](http://img.my.csdn.net/uploads/201707/24/1500885485_3583.png)
  
  图2：RADStack的组件。 kafka作为事件传递端点。 Samza和Hadoop处理数据将数据加载到Druid中。 Druid充当查询的端点。
  
  
  
  ### 3. 服务层
  
  
  
  Druid是专为探索性分析设计的面向列的数据存储，是RADStack中的服务层。 Druid集群由不同类型的节点组成，类似于RADStack的整体设计，每个节点类型都被用来执行一组特定的事物。 我们相信这种设计可以分离出问题，简化整个系统的复杂性。 为了解决复杂的数据分析问题，不同的节点类型结合起来形成一个完整的系统。 Druid 集群中数据的组成和数据流程如图3所示
  
  
  
  ![](http://img.my.csdn.net/uploads/201707/26/1501053095_9850.png)
  
  图3 : Druid集群的概述以及通过集群的数据流。
  
  
  
  #### 3.1 Segment
  
  
  
  Druid中的数据表（称为 **DataSource**）是时间戳记录事件的集合，并被划分为一组 segment，其中每个segment通常为5-10万行。 segment是Druid查询中的基本存储单元。
  
  
  
  Druid始终需要一个时间戳列作为简化数据分发策略，数据保留策略和一级查询修剪的方法。 Druid按照时间戳将其DataSource按照明确定义时间间隔来划分，通常为一小时或一天，并可以进一步对来自其他列的值进行分区，以实现所需的segment大小。 segment 的时间粒度是数据量和时间范围的函数。 具有一年扩展的时间戳的数据集更好地按日进行分区，并且具有一天中分布的时间戳的数据集被更好地按小时分区。
  
  
  
  segment 由 `DataSource标识符` ，`数据的时间间隔`和`每当创建新段时增加的版本字符串唯一标识`。 版本字符串表示段数据的新鲜度; 具有更高版本的片段具有比较旧版本的片段更新的数据视图（在一段时间内）。 该段元数据由系统用于并发控制; **读取操作始终从具有该时间范围的最新版本标识符的段中访问特定时间范围内的数据**。
  
  
  
  Druid段以列方向存储。 鉴于Druid最适用于聚合事件流。 列存储允许更高效的CPU使用，因为只需要实际加载和扫描。 在面向行的数据存储中，与行相关联的所有列都必须作为聚合的一部分进行扫描。 额外的扫描时间可能引起显着的性能下降。
  
  
  
  Druid节点一次使用一个线程扫描一个segment，并行扫描的数据量与集群中可用核心数量直接相关。 segment是不可变的，不变性赋予了一些优势。多个线程可以同时扫描同一段。 这有助于提高读取吞吐量。
  
  
  
  一个简单的查询可能同时扫描数千个segment，并且许多查询可能同时运行。 我们希望确保整个集群在执行单个昂贵查询时不会被饿死。 因此，段在其可以容纳多少数据上具有上限，并且其大小被设置为在几毫秒内被扫描。 通过保持段计算速度非常快，核心和其他资源不断产生。 这样可以确保来自不同查询的分段始终被扫描。
  
  
  
  Druid segment 对于他们所持有的数据的时间间隔非常独立。 列数据直接存储在segment中。 Druid 有多种列类型来表示各种数据格式。 时间戳存储在 long型 列中，维数存储在 String 列中，度量存储在int，float，long或double列中。 根据列类型，可以使用不同的压缩方法。 使用LZ4压缩压缩公制柱。 字符串列是字典编码的，类似于其他数据存储，如 PowerDrill。 可能为特定列创建附加索引。 例如，Druid将默认为字符串倒排倒排索引。
  
  
  
  #### 3.2 Filter
  
  
  
  在一段时间内，实际的OLAP查询通常会要求对某些度量标准进行聚合总计，以过滤某些布尔表达式的维度规格。 表1的示例查询可能会询问：“publisher bieberfever.com和ultratrimfast.com会产生多少次点击和多少收入？”。
  
  
  
  考虑表1中的发布者列，一个字符串列。 对于表1中的每个唯一发布者，我们可以生成一个倒排索引，告诉我们在哪个表行中看到特定的页面。 我们的倒排索引如下所示：
  
  
  
  ```
  
  bieberfever.com -> rows [0, 1, 2] -> [1][1][1][0][0][0]
  
  ultratrimfast.com -> rows [3, 4, 5] -> [0][0][0][1][1][1]
  
  ```
  
  在反向索引中，数组索引表示我们的行，数组值表示是否看到特定的值。 在我们的例子中，bieberfever.com可以在第0行，第1行和第2行看到。要知道哪些行包含bieberfever.com或ultratrimfast.com，我们可以将两个数组进行OR组合。
  
  
  
  ```
  
  [1][1][1][0][0][0] OR [0][0][0][1][1][1] = [1][1][1][1][1][1]
  
  ```
  
  我们的查询是汇总两个指标。 指标值直接存储在一个数组中 :
  
  
  
  ```
  
  
  
  Clicks -> [0, 0, 1, 0, 0, 1]
  
  Price  -> [0.65, 0.62. 0.45, 0.87, 0.99, 1.53]
  
  
  
  ```
  
  我们只能加载我们查询所需的指标列。 我们不需要扫描给定查询的整个度量列。 例如，发布商bieberfever.com的结果查询仅需要在度量列中扫描行 0 , 1 和 2。 倒排索引会精确地告诉我们需要扫描的行，而Druid有一个内部游标，只会访问与最终的倒排索引匹配的行。
  
  
  
  #### 3.3 Streaming Data 导入
  
  
  
  Druid实时节点封装了从数据流中获取，查询和创建segment的功能。 通过这些节点索引的数据可立即进行查询。 节点仅涉及相对较小的时间范围（例如小时）的数据，并且定期地将在这个小时间范围内收集的不可变的批量交给Druid集群中专门处理批量数据的其他节点。 节点使用分布式协调服务（这是Zookeeper），宣布他们的在线状态和他们所服务的数据。
  
  
  
  实时节点采用日志结构合并树来获取最近摄入的数据。 首先将进入的事件存储在内存缓冲区中。 内存中的缓冲区是可直接查询的，并且Druid使用K/V存储，用于存储此内存中的数据。 内存缓冲区是大量写入优化的，并且由于Druid真正用于大量并发读取，数据不会在内存缓冲区中保留很长时间。 实时节点会将其内存中的索引定期或达到某些最大行限制后将其内存索引持久保留。 这个持久化过程将存储在内存缓冲区中的数据转换为3.1节中描述的面向列的段存储格式。 持久化的段被内存映射并加载到非堆内存中，以使它们仍然可以被查询。 这在图4中说明。数据在持久性过程中持续可查询。
  
  
  
  ![](http://img.my.csdn.net/uploads/201707/26/1501072926_9928.png)
  
  
  
  图4：实时节点将事件写入优化的内存中索引。 周期性地将事件持续到磁盘，将写优化格式转换为读优化格式。 在定期的基础上，持续的索引然后合并在一起，最终的分段被切换。 查询将同时打入内存和持久索引。
  
  
  
  
  
  Druid 的实时摄入是自我调节的。 如果在上游数据生成器的数据卷中出现明显的尖峰，则内置几个安全机制。回想一下，事件首先存储在内存缓冲区中，并且在达到最大可配置行限制时会触发召回。 数据卷中的峰值应该会导致更多的召回事件，而不会溢出内存缓冲区。 但是，构建segment的过程需要时间和资源。 如果太多的并发持续触发，并且如果将事件添加到内存缓冲区中的速度比通过持久进程删除的速度更快，那么仍然会出现问题。 Druid 对一次可能发生的最大触发数量设置限制，如果达到此限制，德鲁伊将开始遏制数据摄取。 在这种情况下，面对上升消费者的责任，面对越来越多的积压，这种责任是有弹性的。
  
  
  
  实时节点存储可配置的时间段内的最新数据，通常为一小时。 这个时期被称为段粒度周期。 节点采用滑动窗口接受和拒绝事件，并使用标准时间作为窗口的基础。 在节点的时间范围内的数据被接受，并且该窗口之外的数据被丢弃。 这个时期被称为窗口周期，典型的窗口周期长度为10分钟。 在segment粒度周期加上窗口周期结束时，实时节点将切断其在片段粒度周期期间收集的数据。 使用窗口周期意味着延迟数据可能会丢失。 实际上，我们看到这些事件是罕见的，但它们确实发生了。 Druid 的实时逻辑并不能保证一次成功处理，而是尽力而为。在 Druid 中缺少一次成功处理是在RADStack中需要批量修复的动机之一。
  
  
  
  为了进一步说明，请参见图5. 图5说明了实时节点的操作。 节点从13:37开始，窗口周期为10分钟，仅接受13:27到13:47之间的窗口事件。 当第一个数据被捕获时，节点宣布它在13:00至14:00间隔内服务一个段。 每10分钟（持续时间可配置），节点将刷新并将其内存缓冲区保存到磁盘。 接近尾声，节点可能会在14:00至15:00之间看到事件。 发生这种情况时，节点准备为下一个小时提供数据，并创建一个新的内存缓冲区。 该节点然后宣布，它也在14:00至15:00服务。 在13:10，这是小时的结束加上窗口周期，节点开始切换进程。
  
  
  
  ![](http://img.my.csdn.net/uploads/201707/26/1501073951_1527.png)
  
  
  
  图5 :  节点启动，摄取数据，持续存在，并定期关闭数据。 这个过程无限期地重复。 不同实时节点操作之间的时间段是可配置的。
  
  
  
  #### 3.4 Hand off
  
  
  
  实时节点旨在处理最近数据的小窗口，并需要定期移交他们构建的segment。 切换过程首先涉及压缩步骤。 压缩过程查找在特定时间间隔内创建的所有段（例如，所有由中间持续创建的段在一小时内）。 这些segment被合并在一起，构成了切换的最终不可变sement。
  
  
  
  切换发生在几个步骤。 首先，最终的段被上传到永久备份存储器，通常是分布式文件系统，如S3或HDFS，Druid称之为“**Deepstore**”。 接下来，在 **元数据库**(通常是诸如MySQL的RDBMS)中创建条目以指示已经创建了新的段。 元数据存储中的此条目将最终导致Druid集群中的其他节点下载并服务该段。 实时节点继续服务于该段，直到它注意到在Druid **Historical**节点（其是专用于服务历史数据的节点）上可用的段。 在这一点上，该段被从实时节点中删除并且不被通知。 整个切换过程是流畅的; 在整个切换过程中，数据仍然是可追溯的。 通过实时处理创建的段由段粒度间隔开始进行版本控制。
  
  
  
  #### 3.5 Batch Data 导入
  
  
  
  通过实时摄取使用的核心组件是一个哈希映射，可以逐步填充和定型，以创建一个不可变区段。 这个核心组件在实时和批量摄取之间是共享的。 通过利用Hadoop并运行MapReduce作业来分区数据，Druid已经建立了对创建Segment的支持。 事件可以一次一个读取，直接从静态文件以“流”方式读取。
  
  
  
  类似于实时摄取逻辑，通过批量摄取创建的片段直接上传到 Deepstore。 一旦创建了所有段，Druid的基于Hadoop的批量索引器也将在元数据存储中创建一个条目。
  
  
  
  #### 3.6 Unifying Views
  
  
  
  当在元数据存储中创建新条目时，它们最终将被 Druid **Coordinator** 节点注意到。 Druid Coordinator节点轮询 MetaStore，以便在Druid Historical节点上加载哪些段，并将结果与这些节点上实际加载的结果进行比较。 Coordinator节点将告诉历史节点加载新片段，删除过时片段，并跨节点移动片段。
  
  
  
  Druid Historical节点操作非常简单。 他们知道如何加载，删除和响应扫描片段的查询。 历史节点通常存储大于一小时的所有数据（最近的数据存在于实时节点上）。 实时切换过程要求历史记录必须首先加载并开始为段的查询，然后才能从实时节点中删除该段。 由于段是不可变的，所以段的相同副本可以存在于多个历史节点和实时节点上。 典型生产中的大多数节点德鲁伊集群是历史节点。
  
  
  
  为了整合来自历史和实时节点的结果，Druid有一组作为客户端查询终结点的**Broker**节点。 Broker节点 部分功能是到历史和实时节点的查询路由器。 Broker节点了解在分布式协调服务（Zookeeper）中发布的元数据，可以获取 segment 在 historical 或者 realtime-node 中的位置。 将查询转入正确的历史或实时节点。 broker节点还将historical和realtime-node的部分结果合并，然后将最终的合并结果返回给调用者。
  
  
  
  Broker节点维护段时间轴，其中包含有关群集中存在的段以及这些段的版本的信息。 Druid使用多版本并发控制来管理如何从段中提取数据。 具有较高版本标识符的段具有较低版本标识符的段优先级。 如果两个段在一个间隔内完全重叠，则Druid仅考虑具有较高版本的段的数据。 这在图6中示出。当它们被公布时，段被插入到时间线中。 时间轴根据数据间隔在类似于间隔树的数据结构中对段进行排序。 时间轴中的查找将以与查找间隔重叠的间隔以及段中的数据有效的间隔范围返回所有段。
  
  
  
  ![](http://img.my.csdn.net/uploads/201707/26/1501076004_7558.png)
  
  
  
  图 6 :Druid利用多版本并发控制，并以特定间隔从最新版本的段读取数据。 完全遮蔽的segment被忽略，并最终自动从集群中删除。
  
  
  
  Broker提取查询的间隔，并将其用于查询到时间轴。 时间轴的结果用于将原始查询重新映射到一组具体查询，用于存储相关查询数据的实际历史和实时节点。 历史和实时节点的结果最终由broker合并，该代理将最终结果返回给调用者。
  
  
  
  coordinator 节点还为集群中的segment构建时间轴。 如果某个细分受到一个或多个segment受众群完全遮挡，则会在此时间轴中标记。 当协调员注意到阴影部分时，它告诉历史节点从集群中删除这些段。
  
  
  
  ### 4. 数据转换
  
  
  
  虽然Druid可以一次摄取一个流式传输的事件，但由于Druid不能支持连接查询，因此数据必须事先进行规范化处理。 此外，原始的数据必须经过变换才能被应用程序使用。
  
  
  
  #### 4.1 Stream 处理
  
  
  
  流处理器提供基础设施来开发无界序列消息的处理逻辑。 我们使用 **Apache Samza** 作为我们的流处理器，尽管其他技术是可行的替代方案（我们最初选择了Storm，但已经切换到Samza）。 Samza提供了一个API来编写在一系列**tuples**上运行的作业，并以用户定义的方式对这些tuples执行操作。 每个作业的输入由**Kafka**提供，它也可以作为输出作业的 **sink**。 Samza作业在资源管理和任务执行框架（如YARN）中执行。 进入Kafka/YARN/Samza互动的全部细节超出了本文的范围，但其他文献中也提供了更多的信息[1]。 我们将重点关注如何利用此框架来处理分析用例的数据。
  
  
  
  在Samza基础架构之上，我们引入了一个“**pipeline**”的概念，它是一系列相关处理阶段的分组，其中“上游”阶段产生被“下游”阶段消耗的消息。 我们的一些工作涉及重命名数据，插入空值和空字符串的默认值以及过滤数据等操作。 一个 pipeline 可以在 Druid 中写入多个 datesource。
  
  
  
  要了解一个真实的pipeline，让我们来看一个在线广告的例子。 在线广告中，活动是通过广告的展示次数（观看次数）和广告的点击次数生成的。 许多广告客户有兴趣了解广告转化为点击的展示次数。 曝光流和点击流几乎总是以广告服务器的不同流形式生成。 回想一下，Druid在查询时不支持 Join 操作，因此事件必须在处理时生成。 这两个事件流生成的数据示例如图7所示。每个事件都有一个唯一的 impression id 标识，用于标识所投放的广告。 我们使用这个id作为我们的join 的 key。
  
  
  
  ![](http://img.my.csdn.net/uploads/201707/27/1501138634_4634.png)
  
  
  
  图7：广告展示次数和点击记录在两个不同的流中。 我们要加入的事件位于两个不同Kafka 的不同 topic 中。
  
  
  
  Samza加工管道的第一阶段是一个 shuffle 的步骤。 事件根据事件的 impression id的hash值写入Kafka topic。 这确保了需要加入的事件被写入相同的Kafka topic。 运行Samza任务的YARN容器可能会从一个或多个Kafka topic 中读取，因此对于join 操作来说，Samza任务实际上有两个需要加入的事件。 shuffle 阶段如图8所示。
  
  
  
  ![](http://img.my.csdn.net/uploads/201707/27/1501139988_9211.png)
  
  
  
  图8：shuffle 操作确保在存储在同一个Kafka分区中的事件被连接起来。
  
  
  
  数据管道的下一个阶段是实际加入印象和点击事件。 这是由另一个Samza任务完成的，该任务在数据中创建一个新的名为“is_clicked”的字段。 如果展示事件和具有相同展示ID的点击事件都存在，则此字段标记为“true”。 原始事件被丢弃，新的事件进一步向下游发送。 这个join 阶段 如图9所示
  
  
  
  ![](http://img.my.csdn.net/uploads/201707/27/1501140222_8892.png)
  
  
  
  图 9 : 连接操作添加一个新字段“is_clicked”。
  
  
  
  我们的数据处理的最后阶段是增强数据。 该阶段清理数据中的故障，并执行事件的转换和查找。 一旦数据被清理，就可以将其传送给Druid进行查询。 流数据处理总流程如图10所示。
  
  
  
  ![](http://img.my.csdn.net/uploads/201707/27/1501140807_2955.png)
  
  
  
  图10：流处理数据流水线。
  
  
  
  我们设计的系统并不完美。 因为我们的连接是在窗口中进行，但是数据不能无限缓冲，并不是所有的连接都被保证完成。如果数据因为延迟未到达分配的窗口期间，则不会被 join。 在实践中，这是join操作失败的主要原因, 这意味着我们的流处理层不能保证提供100％准确的结果。 此外，即使没有这种限制，Samza也不提供一次处理语义。 网络连接或节点故障可能导致重复事件。 由于这些原因，我们需要运行一个单独的批处理流程，可以对所摄取的数据进行更准确的转换。
  
  
  
  我们处理流程的最后工作是向 Druid 提供数据。 对于高可用性，来自Samza的处理事件同时发送到两个 realtime-node 。 两个节点都收到相同的数据副本，并且有效地作为彼此的副本。 Druid Broker 可以查询数据的副本。 当数据转为永久的 segment 时 ，两个实时节点竞相交出他们创建的segment。 首先推入Deepstore 的 segment 将是用于 Historical 查询的 segment，一旦该segment 加载到 historical 上，则两个realtime-node都将丢弃此segment。
  
  
  
  #### 4.2 Batch Processing
  
  
  
  我们的批处理流水线由多级MapReduce管道组成。 第一组作业反映了我们的流处理流程，主要负责转换数据，以准备要加载到德鲁伊的数据。 第二组工作负责直接创建不变的Druid segment。 Druid 流媒体和批量摄取的索引代码在两种摄取模式之间是共享的。 然后将这些片段上传到Deepstore并在Metastore中注册。 Druid将继续加载批生成的段。 批处理通常比实时流程运行得少得多，并且可能在传送原始事件后可能运行数小时甚至数天。 为确保原始数据确实完整。等待是必要的.
  
  
  
  批处理生成的Segment由进程的开始时间进行版本控制。 因此，通过批处理创建的segment将具有大于通过实时处理创建的段的版本标识符。 当这些批量创建的segment被加载到集群中时，它们以原子方式替换由其处理的间隔实时处理创建的segment。 因此，批处理完成之后，Druid查询开始反映批量发送的数据，而不是实时发送的数据。
  
  
  
  #### 4.3 Starfire
  
  
  
  为了避免在Hadoop和Samza上编写相同的处理逻辑，我们创建了一个名为 **Starfire** 的库，以帮助在批处理和流处理框架上生成工作流。 Starfire有一个流式的计算模型，即使在批处理模式下运行也是如此。 它被设计为不需要在任何操作模式下加载所有数据，但它提供了需要跨多个数据点（如“**groupBy**”和“**join**”）工作的操作员。 由于这些运算符不能遍历整个数据集，Starfire使用窗口运行它们。
  
  
  
  Starfire程序要求用户必须提供的特定窗口大小（按时间测量，如15分钟或2小时）。 Hadoop驱动程序通过加载用户请求处理的数据的附加数据来提供一个窗口。 Samza驱动程序通过在K/V存储中缓存数据一段时间。 Starfire支持使用不同的窗口大小从从一个程序到另一个程序，如果您正在运行组合批/实时架构并且希望在批量中使用大于实时的窗口，这将非常有用。 我们希望在未来的文学中更多地探索Starfire。
  
  
  
  ### 5. THE DELIVERY LAYER
  
  
  
  在我们的堆栈中，事件通过Http POST请求发送到作为Kafka生产者的前端的接收器。 Kafka是一个具有发布和订阅模式的分布式消息系统。 在高层次上，KafKa在所谓的Topic类别中维护事件或消息。 分布式Kafka集群由许多Broker组成，它们将消息存储在复制的提交日志中。 卡夫卡消费者订阅Topic并处理发布消息的提要。
  
  
  
  Kafka提供数据生产者和数据消费者之间的功能隔离。 发布/订阅模式对于我们的案例很有效，因为多个消费者可以订阅相同的Topic并处理同一组事件。
  
  
  
  我们有两个主要的Kafka消费者。 第一个是Samza的工作，它从Kafka读取消息，用于流处理，如第4.1节所述。 Kafka的Topic映射到Samza的管道，Samza的管道映射到Druid的DataSource。 第二个消费者从Kafka读取消息并将其存储在分布式文件系统中。 该文件系统与用于Druid Deepstore的文件系统相同，也可作为原始事件的备份。 在Deepstore中存储原始事件的目的是使我们可以在任何给定的时间在其上运行批处理作业。 例如，我们的流处理层可以选择在首次处理新管道时不包括某些列。 如果将来要包括这些列，我们可以重新处理原始数据以生成新的德鲁伊段。
  
  
  
  Kafka是进入我们系统的事件的单点交付，必须具有最高的可用性。 我们将多个数据中心的Kafka生产商复制。 如果卡夫卡经纪商和消费者变得无响应，只要我们的HTTP端点仍然可用，我们可以在恢复系统的同时缓冲生产者方面的事件。 同样，如果我们的处理和服务层完全失败，我们可以通过重启Kafka的事件来恢复。
  
  
'''
tags: []
isStarred: false
isTrashed: false
createdAt: "2017-08-31T05:48:30.629Z"
updatedAt: "2018-05-06T03:43:04.824Z"
