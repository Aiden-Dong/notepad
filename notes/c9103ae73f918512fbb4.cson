type: "MARKDOWN_NOTE"
folder: "fec47565a0577152774f"
title: "6- SVM 支持向量机"
content: '''
  #### 6- SVM 支持向量机
  
  ---
  
  转载自:[https://zhuanlan.zhihu.com/p/28660098](https://zhuanlan.zhihu.com/p/28660098), 欢迎阅读原创。
  
  ### 1. 支持条件
  
  ##### 什么是支持向量机
  
  对于线性可分两类数据，支持向量机就是条直线(对于高维数据点就是一个超平面)。
  两类数据点中的的分割线有无数条，SVM就是这无数条中最完美的一条，怎么样才算最完美呢？
  就是这条线距离两类数据点越远，则当有新的数据点的时候我们使用这条线将其分类的结果也就越可信。
  例如下图中的三条直线都可以将A中的数据分类
  
  > 那条可以有最优的分类能力呢？
  
  1. 我们需要线找到数据点中距离分割超平面距离最近的点(找最小)
  2. 然后尽量使得距离超平面最近的点的距离的绝对值尽量的大(求最大)
  
  ![](https://pic2.zhimg.com/50/v2-35a993aa550d744e45bf51c5db896fd5_hd.jpg)
  
  这里的数据点到超平面的距离就是**间隔**(margin), 当间隔越大，我们这条线(分类器)也就越健壮。
  
  那些距离分割平面最近的点就是支持向量(Support Vectors).
  
  
  一句话总结下就是:
  
  **支持向量机就是用来分割数据点那个分割面.
  他的位置是由支持向量确定的(如果支持向量发生了变化，往往分割面的位置也会随之改变), 
  因此这个面就是一个支持向量确定的分类器即支持向量机。**
  
  ### 2. 构建模型
  
  ##### 求解支持向量机
  
  本部分总结如何获取数据集的最优间隔分割平面(支持向量机)。
  
  ##### 分割超平面
  
  将一维直线和二维平面拓展到任意维, 分割超平面可以表示成:
  
  $$w^{T}x + b = 0$$
  
  其中`w`和`b`就是SVM的参数，不同的`w`和`b`确定不同的分割面.
  
  这里我们可以回忆一下Logistic回归，在Logistic回归模型中，我们也同样将 $$z=w^{T}x$$ 放入到sigmoid函数中来做极大似然估计获取最有的参数 `w` ，其中logistic模型中 `w` 中的 `w_{0}` 便对应着现在我们这里的截距 `b` 。
  
  但是与Logistic回归中我们将 $$z=w^{T}x + b$$ 代入到sigmoid函数中获取的值为1或者0也就是数据标签为0或1。
  
  而在SVM中我们对于二分类，不再使用0/1而是使用+1/-1作为数据类型标签。之所以使用+1/-1是为了能方便的使用间隔公式来表示数据点到分割面的间隔。
  
  
  
  ##### 数据点与超平面的间隔
  
  根据数据点到分割超平面的距离公式:
  
  $$d = \\frac{1}{\\lVert w \\rVert} |w^{T}x + b|$$
  
  可见，在距离公式中有两个绝对值，其中分母上是常量，分子上则是与数据点相关的:
  如果数据点在分割平面上方， $$w^{T}x + b > 0$$ ; 
  数据点在分割平面下方， $$w^{T}x + b < 0$$ 。
  
  这样我们在表示任意数据点到分割面的距离就会很麻烦，但是我们通过将数据标签设为+1/-1来讲距离统一用一个公式表示:
  
  $$d = y_{i} \\cdot (w^{T} + b) \\cdot \\frac{1}{\\lVert w \\rVert}$$
  
  这样，当数据点在分割面上方时， $$y_{i}=1$$ , d > 0 且数据点距离分割面越远 d 越大;
  当数据点在分割面下方时， $$y_{i}=-1$$ ,  d 扔大于0, 且数据点距离分割面越远 d 越大。
  
  ##### 目标函数
  
  我们现在已经有了间隔的公式，我们需要找到一组最好的 w 和 b 确定的分割超平面使得**支持向量**距离此平面的**间隔**最大。
  
  ![](https://pic3.zhimg.com/50/v2-54182cabef9f26a2548976d8615a44d6_hd.jpg)
  
  #####  直接形式
  
  直接使用公式表示:
  
  $$arg \\max \\limits_{w, b} \\{ \\min \\limits_{n} (y_{i} \\cdot (w^{T} + b)) \\cdot \\frac{1}{\\lVert w \\rVert} \\}$$
  
  通俗翻译下就是现在数据点中找到距离分割平面最近的点(支持向量)，然后优化**w**和**b**来最大化支持向量到分割超平面的距离。
  
  直接优化上面的式子很困难，我们需要做一些处理，使得同样的优化问题可以使我们用方便的优化算法来求解。
  
  ##### 等比例改变参数 w 和 b
  
  首先看下分割超平面的一个性质。
  当我们等比例的扩大或缩小ww和bb并不会改变超平面的位置。
  例如对于位于三维空间中的二维平面 $$3x + 2y + z + 5 = 0$$ ,$$w=[3, 2, 1]^{T}$$ , b=5 ，我们等比例扩大或者缩小`w`和`b`并不会影响平面.
  即 $$\\frac{3}{2} + y + \\frac{1}{2}z = 0$$ 与原始平面相同。
  
  这样我们就可以任意等比例修改参数，来使我们优化的目标表达起来更加友好。
  
  > 几何间隔和函数间隔
  
  - 函数间隔(Functional Margin):  $$\\hat{\\gamma_{i}} = y_{i}(wx_{i}^{T} + b)$$
  - 几何间隔(Geometry Margin):  $$\\gamma_{i} = y_{i}(\\frac{wx_{i}^{T}}{\\lVert w \\rVert} + \\frac{b}{\\lVert w \\rVert})$$
  
  可见由于我们可以等比例的改变参数，函数间隔相当于参数都乘上了 $$\\lVert w \\rVert$$ .
  
  ##### 标准形式
  
  我们首先考虑一个决策面是否能够将所有的样本都正确分类的约束。图中的样本点分成两类（红色和蓝色），我们为每个样本点 $${x}_i$$  加上一个类别标签$$y_i$$：
  
  $$y_i = \\left\\{\\begin{array}{ll}+1 & \\textrm{for blue points}\\\\-1 & \\textrm{for red points}\\end{array}\\right.$$ (2.7)
  
  如果我们的决策面方程能够完全正确地对图2中的样本点进行分类，就会满足下面的公式
  
  ![](http://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D+%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%3E0+%26+%5Ctextrm%7Bfor%7E%7E%7D+y_i%3D1%5C%5C%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%3C0+%26+%5Ctextrm%7Bfor%7E%7E%7D+y_i%3D-1%5Cend%7Barray%7D%5Cright.)(2.8)
  
  如果我们要求再高一点，假设决策面正好处于间隔区域的中轴线上，并且相应的支持向量对应的样本点到决策面的距离为d，那么公式(2.8)就可以进一步写成：
  
  ![](http://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D+%28%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%29%2F%7C%7C%5Cboldsymbol%7B%5Comega%7D%7C%7C%5Cgeq+d+%26+%5Cforall%7E+y_i%3D1%5C%5C%28%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%29%2F%7C%7C%5Cboldsymbol%7B%5Comega%7D%7C%7C%5Cleq+-d+%26+%5Cforall%7Ey_i%3D-1%5Cend%7Barray%7D%5Cright.)（2.9）
  
  符号$$\\forall$$是“对于所有满足条件的” 的缩写。我们对公式(2.9)中的两个不等式的左右两边除上d，就可得到：
  
  ![](http://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D+%5Cboldsymbol%7B%5Comega%7D_d%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma_d%5Cgeq+1+%26+%5Ctextrm%7Bfor%7E%7E%7D+y_i%3D1%5C%5C%5Cboldsymbol%7B%5Comega%7D_d%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma_d%5Cleq+-1+%26+%5Ctextrm%7Bfor%7E%7E%7D+y_i%3D-1%5Cend%7Barray%7D%5Cright.) (2.10)
  
  其中
  
  ![](http://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Comega%7D_d+%3D+%5Cfrac%7B%5Cboldsymbol%7B%5Comega%7D%7D%7B%7C%7C%5Cboldsymbol%7B%5Comega%7D%7C%7Cd%7D%2C%7E%7E+%5Cgamma_d+%3D+%5Cfrac%7B%5Cgamma%7D%7B%7C%7C%5Cboldsymbol%7B%5Comega%7D%7C%7Cd%7D)
  
  **写成标准形式**:
  
  ![](http://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D+%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%5Cgeq+1+%26+%5Ctextrm%7Bfor%7E%7E%7D+y_i%3D1%5C%5C%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%5Cleq+-1+%26+%5Ctextrm%7Bfor%7E%7E%7D+y_i%3D-1%5Cend%7Barray%7D%5Cright.) (2.11)
  
  
  
  公式(2.11)里面 ![](http://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%3D+1%7E%7E%5Ctextrm%7Bor%7D%7E%7E-1) 的情况什么时候会发生呢，参考一下公式(2.9)就会知道，只有当 $${x}_i$$ 是 ![](http://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D%2B%5Cgamma%3D0)所对应的支持向量样本点时，等于1或-1的情况才会出现。这一点给了我们另一个简化目标函数的启发。回头看看公式(2.6)，你会发现等式右边分子部分的绝对值符号内部的表达式正好跟公式(2.11)中不等式左边的表达式完全一致，无论原来这些表达式是1或者-1，其绝对值都是1。所以对于这些支持向量样本点有：
  
  ![](http://www.zhihu.com/equation?tex=d+%3D+%5Cfrac%7B%7C%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%7C%7D%7B%7C%7C%5Cboldsymbol%7B%5Comega%7D%7C%7C%7D%3D%5Cfrac%7B1%7D%7B%7C%7C%5Cboldsymbol%7B%5Comega%7D%7C%7C%7D%2C%7E%7E%5Ctextrm%7Bif%7D+%7E%5Cboldsymbol%7Bx%7D_i+%5Ctextrm+%7Bis+a+support+vector%7D+) (2.12)
  
  我们的优化目标是最大化数据点到超平面的间隔，这里可以把最小化的部分(寻找支持向量)放到约束条件中, 有
  
  $$arg \\max \\limits_{w, b} \\frac{\\hat{\\gamma}}{\\lVert w \\rVert}$$  约束条件: $$y_{i} \\cdot (wx_{i}^{T} + b) \\ge \\hat{\\gamma}$$, i = 1,2,…,k
  
  意思就是在这个**支持向量**的前提下(点到超平面的距离最短), 使得这个点到超平面的距离最大化。
  
  其中 $$\\hat{\\gamma}$$ 是支持向量到超平面的**函数间隔**。
  
  我们将所有参数 w 和 b 除以 $$\\hat{\\gamma}$$ ，便有 $$\\hat{\\gamma} = \\min y_{i}(wx_{i}^{T} + b) = 1$$ , 于是有:
  
  $$arg \\max \\limits_{w, b} \\frac{1}{\\lVert w \\rVert}$$ 约束条件: $$y_{i} \\cdot (wx_{i}^{T} + b) \\ge 1$$, i = 1,2,…,k
  
  > 将最大化问题转换为求最小值:
  
  $$arg \\min \\limits_{w, b} \\frac{1}{2} \\lVert w \\rVert ^{2}$$ 约束条件: $$y_{i} \\cdot (wx_{i}^{T} + b) \\ge 1$$, i = 1,2,…,k  ----- (2.13)
  
  这便是一个线性不等式约束下的二次优化问题, 下面我本就使用拉格朗日乘子法来获取我们优化目标的对偶形式。
  
  
  ##### 带等式约束的优化问题
  
  添加了等式限制条件，优化函数为: $$\\min \\limits_{x} f(x)$$ 约束条件 $$h(x) = 0$$
  
  拉格朗日乘子法就是通过引入新的位置变量(拉格朗日橙子)将上式的约束条件一起放到目标函数中:
  
  $$L(x, \\lambda) = f(x) + \\lambda h(x)$$, 限制条件 $$h(x) = 0$$
  
  > 通过求解方程组: $$\\nabla L(x, \\lambda) = 0$$; $$h(x) = 0$$ 便可得到局部最小值的必要条件。
  
  $$arg \\min \\limits_{w, b} \\frac{1}{2} \\lVert w \\rVert ^{2}$$ 限制条件 $$y_{i} \\cdot (wx_{i}^{T} + b) -1 \\ge 0$$, $$i = 1,2,...,k$$
  
  对应的拉格朗日函数:
  
  $$L(w, b, \\alpha) = \\frac{1}{2} \\lVert w \\rVert^{2} - \\sum_{i=1}^{N} \\alpha_{i}[y_{i}(w^{T}x_{i} + b) - 1]$$
  
  其中,  $$\\alpha_{i} \\ge 0$$
  
  令 $$g_{i}(w, b) = -y_{i}(w^{T}x_{i} + b) + 1$$，则有 $$g_{i}(w, b) \\le 0$$ .
  
  - 如果 $$\\alpha_{i} \\gt 0$$ , 根据KKT条件 $$\\alpha_{i}g_{i}(w, b) = 0$$ , 推出 $$g_{i}(w, b) = 0$$ , 则约束 $$g_{i} \\le 0$$ 是一个有效约束(active constraint), 对应的 $$x_{i}$$ 为支持向量
  - 如果 $$\\alpha_{i} = 0$$ , 即 $$g_{i}(w, b) \\lt 0$$ , $$g_{i}(w, b) \\le 0$$ 为不起作用的约束，对应的 $$x_{i}$$ 不是支持向量
  
  可见，**支持向量对应的约束为活动约束, 我们的目标函数是由支持向量决定的**，毕竟我们就是支持向量机嘛.
  
  
  
'''
tags: []
isStarred: true
isTrashed: false
createdAt: "2017-11-29T14:24:04.048Z"
updatedAt: "2017-12-11T11:52:14.263Z"
