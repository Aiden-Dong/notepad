type: "MARKDOWN_NOTE"
folder: "fec47565a0577152774f"
title: "6- SVM 支持向量机"
content: '''
  #### 6- SVM 支持向量机
  
  ---
  [http://blog.csdn.net/macyang/article/details/38782399/](http://blog.csdn.net/macyang/article/details/38782399/)
  
  ### 1. 支持条件
  
  ##### 什么是支持向量机
  
  对于线性可分两类数据，支持向量机就是条直线(对于高维数据点就是一个超平面)。
  两类数据点中的的分割线有无数条，SVM就是这无数条中最完美的一条，怎么样才算最完美呢？
  就是这条线距离两类数据点越远，则当有新的数据点的时候我们使用这条线将其分类的结果也就越可信。
  例如下图中的三条直线都可以将A中的数据分类
  
  > 那条可以有最优的分类能力呢？
  
  1. 我们需要线找到数据点中距离分割超平面距离最近的点(找最小)
  2. 然后尽量使得距离超平面最近的点的距离的绝对值尽量的大(求最大)
  
  ![](https://pic2.zhimg.com/50/v2-35a993aa550d744e45bf51c5db896fd5_hd.jpg)
  
  这里的数据点到超平面的距离就是**间隔**(margin), 当间隔越大，我们这条线(分类器)也就越健壮。
  
  那些距离分割平面最近的点就是支持向量(Support Vectors).
  
  
  一句话总结下就是:
  
  **支持向量机就是用来分割数据点那个分割面.
  他的位置是由支持向量确定的(如果支持向量发生了变化，往往分割面的位置也会随之改变), 
  因此这个面就是一个支持向量确定的分类器即支持向量机。**
  
  ### 2. 构建模型
  
  ##### 求解支持向量机
  
  本部分总结如何获取数据集的最优间隔分割平面(支持向量机)。
  
  ##### 分割超平面
  
  将一维直线和二维平面拓展到任意维, 分割超平面可以表示成:
  
  $$w^{T}x + b = 0$$
  
  其中`w`和`b`就是SVM的参数，不同的`w`和`b`确定不同的分割面.
  
  这里我们可以回忆一下Logistic回归，在Logistic回归模型中，我们也同样将 $$z=w^{T}x$$ 放入到sigmoid函数中来做极大似然估计获取最有的参数 `w` ，其中logistic模型中 `w` 中的 `w_{0}` 便对应着现在我们这里的截距 `b` 。
  
  但是与Logistic回归中我们将 $$z=w^{T}x + b$$ 代入到sigmoid函数中获取的值为1或者0也就是数据标签为0或1。
  
  而在SVM中我们对于二分类，不再使用0/1而是使用+1/-1作为数据类型标签。之所以使用+1/-1是为了能方便的使用间隔公式来表示数据点到分割面的间隔。
  
  
  
  ##### 数据点与超平面的间隔
  
  根据数据点到分割超平面的距离公式:
  
  $$d = \\frac{1}{\\lVert w \\rVert} |w^{T}x + b|$$
  
  可见，在距离公式中有两个绝对值，其中分母上是常量，分子上则是与数据点相关的:
  如果数据点在分割平面上方， $$w^{T}x + b > 0$$ ; 
  数据点在分割平面下方， $$w^{T}x + b < 0$$ 。
  
  这样我们在表示任意数据点到分割面的距离就会很麻烦，但是我们通过将数据标签设为+1/-1来讲距离统一用一个公式表示:
  
  $$d = y_{i} \\cdot (w^{T} + b) \\cdot \\frac{1}{\\lVert w \\rVert}$$
  
  这样，当数据点在分割面上方时， $$y_{i}=1$$ , d > 0 且数据点距离分割面越远 d 越大;
  当数据点在分割面下方时， $$y_{i}=-1$$ ,  d 扔大于0, 且数据点距离分割面越远 d 越大。
  
  ##### 目标函数
  
  我们现在已经有了间隔的公式，我们需要找到一组最好的 w 和 b 确定的分割超平面使得**支持向量**距离此平面的**间隔**最大。
  
  ![](https://pic3.zhimg.com/50/v2-54182cabef9f26a2548976d8615a44d6_hd.jpg)
  
  #####  直接形式
  
  直接使用公式表示:
  
  $$arg \\max \\limits_{w, b} \\{ \\min \\limits_{n} (y_{i} \\cdot (w^{T} + b)) \\cdot \\frac{1}{\\lVert w \\rVert} \\}$$
  
  通俗翻译下就是现在数据点中找到距离分割平面最近的点(支持向量)，然后优化**w**和**b**来最大化支持向量到分割超平面的距离。
  
  直接优化上面的式子很困难，我们需要做一些处理，使得同样的优化问题可以使我们用方便的优化算法来求解。
  
  ##### 等比例改变参数 w 和 b
  
  首先看下分割超平面的一个性质。
  当我们等比例的扩大或缩小ww和bb并不会改变超平面的位置。
  例如对于位于三维空间中的二维平面 $$3x + 2y + z + 5 = 0$$ ,$$w=[3, 2, 1]^{T}$$ , b=5 ，我们等比例扩大或者缩小`w`和`b`并不会影响平面.
  即 $$\\frac{3}{2} + y + \\frac{1}{2}z = 0$$ 与原始平面相同。
  
  这样我们就可以任意等比例修改参数，来使我们优化的目标表达起来更加友好。
  
  > 几何间隔和函数间隔
  
  - 函数间隔(Functional Margin):  $$\\hat{\\gamma_{i}} = y_{i}(wx_{i}^{T} + b)$$
  - 几何间隔(Geometry Margin):  $$\\gamma_{i} = y_{i}(\\frac{wx_{i}^{T}}{\\lVert w \\rVert} + \\frac{b}{\\lVert w \\rVert})$$
  
  可见由于我们可以等比例的改变参数，函数间隔相当于参数都乘上了 $$\\lVert w \\rVert$$ .
  
  ##### 标准形式
  
  我们的优化目标是最大化数据点到超平面的间隔，这里可以把最小化的部分(寻找支持向量)放到约束条件中, 有
  
  $$arg \\max \\limits_{w, b} \\frac{\\hat{\\gamma}}{\\lVert w \\rVert}$$  约束条件: $$y_{i} \\cdot (wx_{i}^{T} + b) \\ge \\hat{\\gamma}$$, i = 1,2,…,k
  
  意思就是在这个支持向量的前提下(点到超平面的距离最短), 使得这个点到超平面的距离最大化。
  
  其中 $$\\hat{\\gamma}$$ 是支持向量到超平面的**函数间隔**。
  
  我们将所有参数 w 和 b 除以 $$\\hat{\\gamma}$$ ，便有 $$\\hat{\\gamma} = \\min y_{i}(wx_{i}^{T} + b) = 1$$ , 于是有:
  
  $$arg \\max \\limits_{w, b} \\frac{1}{\\lVert w \\rVert}$$ 约束条件: $$y_{i} \\cdot (wx_{i}^{T} + b) \\ge 1$$, i = 1,2,…,k
  
  将最大化问题转换为求最小值:
  
  $$arg \\min \\limits_{w, b} \\frac{1}{2} \\lVert w \\rVert ^{2}$$ 约束条件: $$-y_{i} \\cdot (wx_{i}^{T} + b) \\ge 1$$, i = 1,2,…,k
  
  这便是一个线性不等式约束下的二次优化问题, 下面我本就使用拉格朗日乘子法来获取我们优化目标的对偶形式。
  
  
  ### 3. 模型优化 
  
  ##### 通过拉格朗日乘子法
  
  拉格朗日乘数法是一种寻找多元函数在其变量受到一个或多个条件的约束时的极值的方法。
  这种方法可以将一个有n个变量与k个约束条件的最优化问题转换为一个解有 n+k 个变量的方程组的解的问题。
  这样我们可以将我们带约束的目标函数通过拉格朗日乘子法将约束放入到目标函数中方便优化。
  KKT条件是拉格朗日乘子法在约束条件为不等式的一种延伸。
  下面我就对拉格朗日乘子法和KKT条件进行下简单总结。
  
  ##### 拉格朗日乘子法和KKT条件
  
  我们从无约束优化到带有不等式约束条件逐渐介绍下几种不同类型的优化问题。
  
  ##### 无约束优化问题
  
  对于无约束优化问题
  
  $$\\min \\limits_{x} f(x)$$
  
  梯度 $$\\nabla f(x) = 0$$ 是局部最小点的必要条件，这样，优化问题的求解变成了对该必要条件解方程组。
  
  
  ##### 带等式约束的优化问题
  
  添加了等式限制条件，优化函数为: $$\\min \\limits_{x} f(x) , subject to h(x) = 0$$
  
  拉格朗日乘子法就是通过引入新的位置变量(拉格朗日橙子)将上式的约束条件一起放到目标函数中:
  
  $$L(x, \\lambda) = f(x) + \\lambda h(x)$$, 限制条件 $$h(x) = 0$$
  
  通过求解方程组: $$\\nabla L(x, \\lambda) = 0$$; $$h(x) = 0$$ 便可得到局部最小值的必要条件。
  
  ##### 拉格朗日乘子法原理
  
  我在这里稍微总结下Lagrange Multiplier的原理吧.
  
  ![](https://pic1.zhimg.com/50/v2-42bb76fc7eb2c2fdcb984e4b94107c9c_hd.jpg)
  
  参考上图(忽略函数形式的差异)，当约束条件 $$h(x)=0$$ 与 f(x) 的等高线相切的时候，切点具有局部最优值。
  
  此时 $$h(x^{*})$$ 的梯度与 $$f(x^{*})$$ 梯度同向，我们可以加入一个参数 $$\\lambda$$ ，得到他们之间的关系:
  
  $$\\nabla f(x^{*}) + \\lambda \\nabla h(x^{*}) = 0$$
  
  上式便是对 $$L(x, \\lambda) = f(x) + \\lambda h(x)$$ 求梯度等于0的结果, 这也就是拉格朗日函数了，其中那个关联梯度方向的 $$\\lambda$$ 就是拉格朗日乘子.
'''
tags: []
isStarred: false
isTrashed: false
createdAt: "2017-11-29T14:24:04.048Z"
updatedAt: "2017-12-04T12:09:55.739Z"
