type: "MARKDOWN_NOTE"
folder: "7671e01e5a4a4ee04aa9"
title: "MapReduce篇 (08) MapReduce特性"
content: '''
  ### MapReduce篇 (08) MapReduce特性
  
  ---
  
  #### 1- 计数器
  
  > 计数器是收集作业统计信息的有效手段， 用于质量控制或应用级统计。
  
  - #### 内置计数器
  - ##### 任务计数器
  
  > - **任务计数器**由其关联任务维护， 并定期发送给 **tasktracker** , 再由 **tasktracker** 发送给 **jobtracker**。
  > - **任务计数器**的值每次都是完整传输的， 而非自上次传输后再继续尚未完成的传输，从而避免由于消息丢失而引发的错误。
  
  **内置的MapReduce任务计数器: TaskCounter**
  
  ```
  MAP_INPUT_RECORDS       - 作业中所有Map已处理的输入记录值
  MAP_INPUT_BYTES         - 作业中所有Map已处理的未经压缩的输入数据的字节数
  MAP_SKIPPED_RECORDS     - 作业中map跳过的记录数
  MAP_OUTPUT_RECORDS      - 作业中map以输出的记录数
  MAP_OUTPUT_BYTES        - 作业中map已经输出的未经压缩的输出数据字节数
  
  COMBINE_INPUT_RECORDS       - combiner 已处理的输入记录数
  COMBINE_OUTPUT_RECORDS      - combiner 已经处理的输出记录数
  
  REDUCE_INPUT_GROUPS        - reduce 读入的组数
  REDUCE_INPUT_RECORDS       - 作业中所有Reduce已处理的输入记录值
  REDUCE_SKIPPED_RECORDS     - 作业中Reduce跳过的记录数
  REDUCE_OUTPUT_RECORDS      - 作业中Reduce输出的记录数
  ```
  
  > ##### 程序案例
  
  ```
  package count;
  
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.conf.Configured;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.NullWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.*;
  import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
  import org.apache.hadoop.util.Tool;
  import org.apache.hadoop.util.ToolRunner;
  
  import java.io.IOException;
  
  /**
   * Created by saligia on 16-11-13.
   */
  public class SysCountTest extends Configured implements Tool{
      public static class Map extends Mapper<LongWritable, Text, NullWritable, Text> {
          @Override
          protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
              context.write(NullWritable.get(), value);
          }
      }
  
      public static class Reduce extends Reducer<NullWritable, Text, NullWritable,NullWritable> {
          @Override
          protected void setup(Context context) throws IOException, InterruptedException {
              System.out.println("物理内存 时间 :" + context.getCounter(TaskCounter.PHYSICAL_MEMORY_BYTES).getValue());
              System.out.println("虚拟内存 时间 :" + context.getCounter(TaskCounter.VIRTUAL_MEMORY_BYTES).getValue());
          }
  
          @Override
          protected void reduce(NullWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
              Counter counter = context.getCounter("one", "row");
              System.out.println("==============================================");
              for(Text it : values)
                  context.write(NullWritable.get(), NullWritable.get());
              }
  
          }
  
      @Override
      public int run(String[] strings) throws Exception {
          String pathIn = "hdfs://localhost:9000/user/saligia/input/boot.log";
          String pathOut = "hdfs://localhost:9000/user/saligia/output/count-boot";
  
          Configuration conf = new Configuration();
          Job job = Job.getInstance(conf, "Count_job");
  
          job.setJarByClass(SysCountTest.class);
  
          TextInputFormat.addInputPath(job, new Path(pathIn));
          TextOutputFormat.setOutputPath(job, new Path(pathOut));
  
          job.setMapperClass(RowCountTest.Map.class);
          job.setReducerClass(RowCountTest.Reduce.class);
  
          job.setMapOutputKeyClass(NullWritable.class);
          job.setMapOutputValueClass(Text.class);
          job.setOutputKeyClass(NullWritable.class);
          job.setOutputValueClass(NullWritable.class);
  
          int res = job.waitForCompletion(true) ? 1 : 0;
          Counters counters = job.getCounters();
  
          System.out.println("Map 输入：" + counters.findCounter(TaskCounter.MAP_INPUT_RECORDS).getValue());
          System.out.println("Map 输出:" + counters.findCounter(TaskCounter.MAP_OUTPUT_RECORDS).getValue());
          System.out.println("Reduce 输入:" + counters.findCounter(TaskCounter.REDUCE_INPUT_RECORDS).getValue());
          System.out.println("Reduce 输出:" + counters.findCounter(TaskCounter.REDUCE_OUTPUT_RECORDS).getValue());
  
          return res;
      }
  
      public static void main(String[] args) throws Exception {
          int res = ToolRunner.run(new SysCountTest(), args);
  
  
          System.exit(res);
      }
  }
  ```
  
  - ##### 作业计数器 : JobCounter
  
  > - 作业计数器由 jobtracker(或者 YARN 中的 APPMast) 维护， 因此无须在网络间传输数据
  > - 这些计数器都是作业级别的统计量， 其值不会随着任务的运行而改变。
  
  ```
  TOTAL_LAUNCHED_MAPS      - 启动的MAP任务数
  TOTAL_LAUNCHED_REDUCES   - 启动的REDUCE任务数
  TOTAL_LAUNCHED_UBERTASKS - 启动的 uber 任务数
  UNM_UBER_SUBMAPS         - 在uber任务中的map数量
  UNM_UBER_SUBREDUCES      - 在uber任务中的reduce数量
  UNM_FAILED_MAPS          - 失败的map任务数
  UNM_FAILED_REDUCES       - 失败的reduce任务数
  UNM_FAILED_UBER          - 失败的 uber 任务数
  ```
  
  - #### 用户定义的 Java 计数器
  
  > - MapReduce 允许用户编写程序来定义计数器， 计数器的值可以在 mapper 或 reduce 中增加。
  > - 计数器由一个 java 枚举类型来定义,各枚举类型所包含的字段数量也不限。
  > - 枚举的名称为组名称, 枚举的字段为计数器名称。
  
  - ##### 统计计数器
  - ##### 易读的计数器名称
  - ##### 获取计数器值
  
  ```
  除了通过 web 界面和命令行(hadoop job -counter)之外， 用户还可以使用 Java API 来获取计数器的值
  ```
  
  > 使用枚举来实现
  
  ```
  package count;
  
  import org.apache.commons.lang.enums.Enum;
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.conf.Configured;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.NullWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.*;
  import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
  import org.apache.hadoop.util.Tool;
  import org.apache.hadoop.util.ToolRunner;
  
  import java.io.IOException;
  
  /**
  * Created by saligia on 16-11-13.
  */
  public class StaticCounterTest extends Configured implements Tool{
  
  private enum MyselCounter{      // 组名
      M_ERROR,                    // 计数器名
      M_LINE                      // 计数器名
  };
  
  public static class Map extends Mapper<LongWritable, Text, NullWritable, Text> {
      @Override
      protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
          context.write(NullWritable.get(), value);
      }
  }
  
  public static class Reduce extends Reducer<NullWritable, Text, NullWritable,NullWritable> {
      @Override
      protected void reduce(NullWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
          for(Text it : values){
              System.out.println("==============================================");
              context.getCounter(MyselCounter.M_LINE).increment(1);   // 计数器增一
              context.write(NullWritable.get(), NullWritable.get());
          }
  
      }
  
  }
  
  @Override
  public int run(String[] strings) throws Exception {
      String pathIn = "hdfs://localhost:9000/user/saligia/input/boot.log";
      String pathOut = "hdfs://localhost:9000/user/saligia/output/count-boot";
  
      Configuration conf = new Configuration();
      Job job = Job.getInstance(conf, "Count_job");
  
      job.setJarByClass(StaticCounterTest.class);
  
      TextInputFormat.addInputPath(job, new Path(pathIn));
      TextOutputFormat.setOutputPath(job, new Path(pathOut));
  
      job.setMapperClass(Map.class);
      job.setReducerClass(Reduce.class);
  
      job.setMapOutputKeyClass(NullWritable.class);
      job.setMapOutputValueClass(Text.class);
      job.setOutputKeyClass(NullWritable.class);
      job.setOutputValueClass(NullWritable.class);
  
      int res = job.waitForCompletion(true) ? 1 : 0;
      Counters counters = job.getCounters();
      System.out.println("计数器值 : " + counters.findCounter(MyselCounter.M_LINE).getValue());
      return res;
  }
  
  public static void main(String[] args) throws Exception {
      int res = ToolRunner.run(new StaticCounterTest(), args);
      System.exit(res);
  }
  }
  
  ```
  
  > 动态计数器实例
  
  ```
  package count;
  
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.conf.Configured;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.NullWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.*;
  import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
  import org.apache.hadoop.util.Tool;
  import org.apache.hadoop.util.ToolRunner;
  
  import java.io.IOException;
  
  /**
  * Created by saligia on 16-11-13.
  */
  public class RowCountTest extends Configured implements Tool{
  public static class Map extends Mapper<LongWritable, Text, NullWritable, Text>{
      @Override
      protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
          context.write(NullWritable.get(), value);
      }
  }
  
  public static class Reduce extends Reducer<NullWritable, Text, NullWritable,NullWritable>{
      @Override
      protected void reduce(NullWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
          Counter counter = context.getCounter("one", "row");
  
          for(Text it : values){
              counter.increment(1);
              context.write(NullWritable.get(), NullWritable.get());
          }
  
      }
  }
  @Override
  public int run(String[] strings) throws Exception {
      String pathIn = "hdfs://localhost:9000/user/saligia/input/boot.log";
      String pathOut = "hdfs://localhost:9000/user/saligia/output/count-boot";
  
      Configuration conf = new Configuration();
      Job job = Job.getInstance(conf, "Count_job");
  
      job.setJarByClass(RowCountTest.class);
  
      TextInputFormat.addInputPath(job, new Path(pathIn));
      TextOutputFormat.setOutputPath(job, new Path(pathOut));
  
      job.setMapperClass(Map.class);
      job.setReducerClass(Reduce.class);
  
      job.setMapOutputKeyClass(NullWritable.class);
      job.setMapOutputValueClass(Text.class);
      job.setOutputKeyClass(NullWritable.class);
      job.setOutputValueClass(NullWritable.class);
  
      int res = job.waitForCompletion(true) ? 1 : 0;
  
      // 打印显示 计数器信息
  
      Counters counters = job.getCounters();
      Counter count = counters.findCounter("one","row");
      System.out.println(count.getValue());
  
      return res;
  }
  
  public static void main(String[] args) throws Exception {
      int res = ToolRunner.run(new RowCountTest(), args);
  
      System.exit(res);
  }
  }
  
  ```
  
  #### 2-排序
  
  > 排序是 MapReduce 的核心技术， 尽管应用本身可能并不需要对数据进行数据排序， 但人可能使用 MapReduce 的排序功能来组织数据
  
  > ##### 自定义Writable 的实现， 在sort 阶段根据自己的需求进行 排序(调用 compareTo() 的方法)
  
  
  ```
  package sort;
  
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.WritableComparable;
  
  import java.io.DataInput;
  import java.io.DataOutput;
  import java.io.IOException;
  
  /**
   * Created by saligia on 16-11-12.
   */
  public class MyselfWritable implements WritableComparable<MyselfWritable>{
      private IntWritable first;
      private IntWritable second;
  
      public MyselfWritable(){
          first = new IntWritable();
          second = new IntWritable();
      }
      public MyselfWritable(IntWritable first, IntWritable second){
          this.first = first;
          this.second = second;
      }
      @Override
      public int compareTo(MyselfWritable o) {
  
          if(this.first.compareTo(o.first) > 0){
              return 1;
          }else if (this.first.compareTo(o.first) < 0){
              return -1;
          }else {
              if(this.second.compareTo(o.second) > 0)
                  return 1;
              else if(this.second.compareTo(o.second) < 0)
                  return  -1;
          }
          return 0;
      }
  
      @Override
      public void write(DataOutput dataOutput) throws IOException {
          first.write(dataOutput);
          second.write(dataOutput);
      }
  
      @Override
      public void readFields(DataInput dataInput) throws IOException {
          first.readFields(dataInput);
          second.readFields(dataInput);
      }
  
      public void setFirst(IntWritable first){
          this.first = first;
      }
      public void setSecond(IntWritable second){
          this.second = second;
      }
      public IntWritable getFirst(){
          return this.first;
      }
      public IntWritable getSecond(){
          return  this.second;
      }
  }
  ```
  
  > #### 可以重写Reduce端分组过程
  
  ```
  package sort;
  
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.WritableComparable;
  import org.apache.hadoop.io.WritableComparator;
  
  /**
   * Created by saligia on 16-11-12.
   */
  public class MyselfGroup extends WritableComparator{
  
      protected MyselfGroup(){
          super(MyselfWritable.class, true);
      }
  
      @Override
      public int compare(WritableComparable a, WritableComparable b) {
          MyselfWritable one = (MyselfWritable)a;
          MyselfWritable two = (MyselfWritable)b;
  
          IntWritable t1 = one.getFirst();
          IntWritable t2 = two.getFirst();
  
          return  t1.compareTo(t2);
      }
  }
  
  ```
  
  > ##### 重写 Partation 过程
  
  ```
  package sort;
  
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.mapreduce.Partitioner;
  
  /**
   * Created by saligia on 16-11-12.
   */
  public class MysqlPart extends Partitioner<MyselfWritable, IntWritable>{
      @Override
      public int getPartition(MyselfWritable myselfWritable, IntWritable intWritable, int i) {
          if(myselfWritable.getFirst().get() > 10)
              return 1;
  
          return 0;
      }
  }
  
  ```
  
  > #### 调用实例
  
  ```
  package sort;
  
  import join.TableJoin;
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.conf.Configured;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Job;
  import org.apache.hadoop.mapreduce.Mapper;
  import org.apache.hadoop.mapreduce.Reducer;
  import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
  import org.apache.hadoop.util.Tool;
  import org.apache.hadoop.util.ToolRunner;
  
  import java.io.IOException;
  
  /**
   * Created by saligia on 16-11-12.
   */
  public class SortProgress extends Configured implements Tool{
  
      public static class Map extends Mapper<LongWritable, Text, MyselfWritable, IntWritable>{
  
          @Override
          protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
              try{
                  String[] proStr = value.toString().split(" ");
                  // System.out.println(value.toString());
                  int a = Integer.parseInt(proStr[0]);
                  int b = Integer.parseInt(proStr[1]);
  
                  MyselfWritable obj = new MyselfWritable(new IntWritable(a), new IntWritable(b));
                  context.write(obj, new IntWritable(b));
              }catch (IOException e){
                  e.printStackTrace();
              }
  
          }
      }
  
      public static class Reduce extends Reducer<MyselfWritable, IntWritable, IntWritable, IntWritable>{
  
          @Override
          protected void reduce(MyselfWritable key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
              IntWritable first = key.getFirst();
  
              for(IntWritable it : values){
                  // System.out.println(first + " : " + it);
                  context.write(first,it);
              }
          }
      }
  
      @Override
      public int run(String[] strings) throws Exception {
  
          String inPath = "hdfs://localhost:9000/user/saligia/input/data";
          String outPath="hdfs://localhost:9000/user/saligia/output/data";
  
          Configuration conf = new Configuration();
          Job job = Job.getInstance(conf, "sort mapred");
  
          job.setJarByClass(SortProgress.class);
  
          TextInputFormat.addInputPath(job, new Path(inPath));
          TextOutputFormat.setOutputPath(job, new Path(outPath));
  
          job.setMapperClass(Map.class);
          job.setReducerClass(Reduce.class);
          job.setGroupingComparatorClass(MyselfGroup.class);
          job.setPartitionerClass(MysqlPart.class);
  
          job.setMapOutputKeyClass(MyselfWritable.class);
          job.setMapOutputValueClass(IntWritable.class);
          job.setOutputKeyClass(IntWritable.class);
          job.setMapOutputValueClass(IntWritable.class);
          job.setNumReduceTasks(2);
          return job.waitForCompletion(true) ? 1 : 0;
      }
  
      public static void main(String[] args) throws Exception {
          int res = ToolRunner.run(new SortProgress(), args);
          System.exit(res);
      }
  }
  
  ```
  
  #### 3- 多路径输出
  
  > MapReduce 多路径输出使用 **MultipleOutputs** 在 Reduce 来实现。
  
  ```
  MultipleOutputs.write(String namedOutput, K key, V value, String baseOutputPath) ;
  
  -- namedOutput 需要调用在job端加入的 名称空间命名, baseOutputpath 是当前输出路经下的目录名+文件前缀。
  ```
  
  
  
  ```
  package mutil;
  
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.conf.Configured;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Job;
  import org.apache.hadoop.mapreduce.Mapper;
  import org.apache.hadoop.mapreduce.Reducer;
  import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
  import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
  import org.apache.hadoop.util.Tool;
  import org.apache.hadoop.util.ToolRunner;
  
  import java.io.IOException;
  
  /**
   * Created by saligia on 16-11-13.
   */
  public class MutilOutput extends Configured implements Tool{
  
      public static class Map extends Mapper<LongWritable, Text, Text, Text> {
          @Override
          protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
              if(value.toString().contains("Started")){
                  context.write(new Text("Started"), value);
              }else if(value.toString().contains("Starting")){
                  context.write(new Text("Starting"), value);
              }
          }
      }
      public static class Reduce extends Reducer<Text, Text, Text, Text>{
          private MultipleOutputs<Text, Text> mu = null;
          @Override
          protected void setup(Context context) throws IOException, InterruptedException {
              mu = new MultipleOutputs<>(context);
          }
  
          @Override
          protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
              for(Text test : values){
                  if(key.toString().equals("Started")){
                      mu.write("Started", key, test, "started/part");
                  }else if(key.toString().equals("Starting")){
                      mu.write("Starting", key, test, "starting/part");
                  }
              }
          }
      }
      @Override
      public int run(String[] strings) throws Exception {
          String pathIn = "hdfs://localhost:9000/user/saligia/input/boot.log";
          String pathOut= "hdfs://localhost:9000/user/saligia/output/boot";
  
          Configuration conf = new Configuration();
          Job job = Job.getInstance(conf, "Multi Output");
  
          job.setJarByClass(MutilOutput.class);
  
          TextInputFormat.addInputPath(job,new Path(pathIn));
          TextOutputFormat.setOutputPath(job, new Path(pathOut));
  
          job.setMapperClass(Map.class);
          job.setReducerClass(Reduce.class);
  
          MultipleOutputs.addNamedOutput(job, "Started", TextOutputFormat.class, Text.class, Text.class);
          MultipleOutputs.addNamedOutput(job, "Starting", TextOutputFormat.class, Text.class, Text.class);
  
          job.setMapOutputKeyClass(Text.class);
          job.setMapOutputValueClass(Text.class);
          job.setOutputKeyClass(Text.class);
          job.setOutputValueClass(Text.class);
  
          return job.waitForCompletion(true) ? 1 : 0;
      }
  
      public static void main(String[] args) throws Exception {
          int res = ToolRunner.run(new MutilOutput(), args);
          System.exit(res);
      }
  }
  
  ```
  > 生成结果
  
  ```
  [saligia@Norma]:$ hadoop fs -ls output/boot
  Found 4 items
  -rw-r--r--   3 saligia saligia          0 2016-11-13 11:24 output/boot/_SUCCESS
  -rw-r--r--   3 saligia saligia          0 2016-11-13 11:24 output/boot/part-r-00000
  drwxr-xr-x   - saligia saligia          0 2016-11-13 11:24 output/boot/started
  drwxr-xr-x   - saligia saligia          0 2016-11-13 11:24 output/boot/starting
  
  ```
  
  #### 4- 参数传递
  
  > ##### 参数传递类
  
  ```
  package paramset;
  
  import org.apache.hadoop.conf.Configuration;
  import org.codehaus.jettison.json.JSONException;
  
  import java.io.BufferedReader;
  import java.io.FileReader;
  import java.io.IOException;
  import java.util.HashMap;
  
  /**
   * Created by saligia on 16-11-13.
   */
  public class FileLoad {
      public static HashMap<String, String> encode(String args){
          HashMap<String, String> map = new HashMap();
          String[] lineStrs = args.split("\\t");
  
          for(String lineStr : lineStrs){
              String[] fields = lineStr.split(",");
              map.put(fields[0],fields[1]);
          }
          return map;
      }
      public static void load(Configuration conf, String path) throws IOException {
          BufferedReader fin = new BufferedReader(new FileReader(path));
          String outStr = "";
  
          try{
              while(fin.ready()){
                  outStr += fin.readLine()+'\\t';
              }
              outStr = outStr.substring(0, outStr.length()-1);
              conf.set("mylevel", outStr);
          }catch(IOException e){
              e.printStackTrace();
          }finally {
              fin.close();
          }
      }
  }
  ```
  
  > ##### Mapreduce 类
  
  ```
  package paramset;
  
  import join.TableJoin;
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.conf.Configured;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Job;
  import org.apache.hadoop.mapreduce.Mapper;
  import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
  import org.apache.hadoop.util.Tool;
  import org.apache.hadoop.util.ToolRunner;
  
  import java.io.IOException;
  import java.util.HashMap;
  
  /**
   * Created by saligia on 16-11-13.
   */
  public class MRParameterTest extends Configured implements Tool{
  
      public static class Map extends Mapper<LongWritable, Text, Text, Text>{
          private HashMap<String, String> map = null;
          @Override
          protected void setup(Context context) throws IOException, InterruptedException {
              Configuration conf = context.getConfiguration();
              map = FileLoad.encode(conf.get("mylevel"));
              System.out.println(map);
          }
  
          @Override
          protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
              String[] fields = value.toString().split(",");
  
              context.write(new Text(map.get(fields[0])), new Text(fields[1]));
          }
      }
  
      @Override
      public int run(String[] strings) throws Exception {
          String pathIn = "hdfs://localhost:9000/user/saligia/input/data";
          String pathOut = "hdfs://localhost:9000/user/saligia/output/param";
  
          Configuration conf = new Configuration();
          FileLoad.load(conf, "/home/saligia/tmp/conf");
  
          Job job = Job.getInstance(conf, "table-join");
  
          job.setJarByClass(MRParameterTest.class);
  
          TextInputFormat.addInputPath(job, new Path(pathIn));
          TextOutputFormat.setOutputPath(job, new Path(pathOut));
  
          job.setMapperClass(Map.class);
  
          job.setOutputKeyClass(Text.class);
          job.setOutputValueClass(Text.class);
  
          return job.waitForCompletion(true)? 1 : 0;
      }
  
      public static void main(String[] args) throws Exception {
          int res = ToolRunner.run(new MRParameterTest(), args);
          System.exit(res);
      }
  }
  
  ```
  
  
  #### 4- Map join
  
  
  > ##### FileLoad.java
  
  ```
  package test;
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.fs.FileSystem;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.mapreduce.Mapper;
  import org.apache.hadoop.mapreduce.filecache.DistributedCache;
  
  import java.io.*;
  import java.net.URI;
  import java.util.HashMap;
  
  public class FileLoad {
      public static HashMap<String, String> encode(String args){
          HashMap<String, String> map = new HashMap();
          String[] lineStrs = args.split("\\t");
  
          for(String lineStr : lineStrs){
              String[] fields = lineStr.split(",");
              map.put(fields[0],fields[1]);
          }
          return map;
      }
      // configure 加载过程
      /*
      *   ke1,value1\\tke2,value2
       */
      public static void load(Configuration conf, String path) throws IOException {
          BufferedReader fin = new BufferedReader(new FileReader(path));
          String outStr = "";
  
          try{
              while(fin.ready()){
                  outStr += fin.readLine()+'\\t';
              }
              outStr = outStr.substring(0, outStr.length()-1);
              conf.set("mylevel", outStr);
          }catch(IOException e){
              e.printStackTrace();
          }finally {
              fin.close();
          }
      }
  
      public static HashMap<String, String> loadTable(Mapper.Context context) throws IOException {
          HashMap<String, String> tabMap = new HashMap<String, String>();
  
          Path[] filePaths = DistributedCache.getLocalCacheFiles(context.getConfiguration());
          FileSystem fs = FileSystem.get(URI.create(filePaths[0].toString()), context.getConfiguration());
  
          InputStream In = fs.open(filePaths[0]);
          BufferedReader fin = new BufferedReader(new InputStreamReader(In));
  
          try{
              while(fin.ready()){
                  String lineStr = fin.readLine();
                  String[] args = lineStr.split(",");
                  tabMap.put(args[0],args[1]);
              }
          }catch (IOException e){
              e.printStackTrace();
          }finally {
              fin.close();
          }
          return tabMap;
      }
  }
  
  ```
  
  > ##### MapJoinTest.java
  
  ```
  package test;
  
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.conf.Configured;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Job;
  import org.apache.hadoop.mapreduce.Mapper;
  import org.apache.hadoop.mapreduce.filecache.DistributedCache;
  import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
  import org.apache.hadoop.mapreduce.v2.TestMRJobs;
  import org.apache.hadoop.util.Tool;
  import org.apache.hadoop.util.ToolRunner;
  import paramset.*;
  
  import java.io.IOException;
  import java.net.URI;
  import java.util.HashMap;
  
  /**
   * name:level:addr
   */
  public class MapJoinTest extends Configured implements Tool{
  
      public static class Map extends Mapper<LongWritable, Text, IntWritable, Text>{
          private HashMap<String, String> confMap = null;
          private HashMap<String, String> tabMap = null;
  
          @Override
          protected void setup(Context context) throws IOException, InterruptedException {
              Configuration conf = context.getConfiguration();
              confMap = FileLoad.encode(conf.get("mylevel"));
              tabMap = FileLoad.loadTable(context);
              System.out.println(tabMap);
          }
  
          @Override
          protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
              String[] items = value.toString().split(",");
              //Thread.sleep(1000000);
              System.out.println(items[1] + " " + confMap.get(items[0]) + tabMap.get(items[0]));
              context.write(new IntWritable(Integer.parseInt(items[0])), new Text(items[1] + " " + confMap.get(items[0]) + " " + tabMap.get(items[0])));
          }
      }
      @Override
      public int run(String[] strings) throws Exception {
          String pathIn = "hdfs://192.168.1.86:9000/user/saligia/input/data";
          String pathOut = "hdfs://192.168.1.86:9000/user/saligia/output/mapjoin";
          String memTab = "hdfs://192.168.1.86:9000/user/saligia/input/addr";
  
  
          Configuration conf = new Configuration();
  
          // level
          FileLoad.load(conf, "/home/saligia/tmp/conf");
          // addr
          DistributedCache.createSymlink(conf);
          String uriTab = new Path(memTab).toUri().toString() + "#" + "tab.py";
          DistributedCache.addCacheFile(new URI(memTab), conf);
          System.out.println(uriTab);
          //System.exit(0);
          Job job = Job.getInstance(conf, "MapJoin");
  
          job.setJarByClass(MapJoinTest.class);
  
          TextInputFormat.addInputPath(job,new Path(pathIn));
          TextOutputFormat.setOutputPath(job,new Path(pathOut));
  
          job.setMapperClass(Map.class);
          job.setOutputKeyClass(IntWritable.class);
          job.setOutputValueClass(Text.class);
  
          return job.waitForCompletion(true) ? 1 : 0;
      }
  
      public static void main(String[] args) throws Exception {
          int res = ToolRunner.run(new MapJoinTest(), args);
          System.exit(res);
      }
  }
  
  ```
'''
tags: []
isStarred: false
isTrashed: false
createdAt: "2017-09-20T11:46:16.695Z"
updatedAt: "2018-11-19T06:47:19.609Z"
