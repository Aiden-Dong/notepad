type: "MARKDOWN_NOTE"
folder: "ef3e1850a2ffff04209d"
title: "5-hive 源码分析之开始"
content: '''
  ### 5-hive 源码分析之开始
  
  ---
  
  转载自：[http://blog.csdn.net/wzq6578702/article/details/71331123](http://blog.csdn.net/wzq6578702/article/details/71331123)　欢迎阅读原作者内容。
  参考自:[https://www.iteblog.com/archives/1527.html](https://www.iteblog.com/archives/1527.html) 欢迎阅读原作者内容.
  
  - 版本 tag:**release-1.2.1**
  - 编译 : `mvn clean package -Phadoop-2 -DskipTests -Pdist`
  - 程序包: `apache-hive-1.2.1-bin.tar.gz`
  
  
  #### 什么是Hive:
  
  - 数据仓库：存储、查询、分析大规模数据 
  - SQL语言：简单易用的类SQL查询语言 
  - 编程模型：允许开发者自定义UDF、Transform、Mapper、Reducer，来更简单地完成复杂MapReduce无法完成的工作 
  - 数据格式：处理Hadoop上任意数据格式的数据，或者使用优化的格式存储Hadoop上的数据，RCFile，ORCFile，Parquest 
  - 数据服务：HiveServer2，多种API访问Hadoop上的数据，JDBC，ODBC 
  - 元数据服务：数据什么样，数据在哪里，Hadoop上的唯一标准
  
  #### Hive与Hadoop 的关系:
  
  > 整体过程:
  ![image.png](https://upload-images.jianshu.io/upload_images/10402860-b1f057637a293522.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
  
  > 运行机制：
  
  ![image.png](https://upload-images.jianshu.io/upload_images/10402860-30901a35ea9c94a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
  
  > hive cli:
  
  ![image.png](https://upload-images.jianshu.io/upload_images/10402860-4c85605ef17882b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
  
  
  > 架构层面：
  
  ![image.png](https://upload-images.jianshu.io/upload_images/10402860-ef4a94df18e5bbb4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
  
  
  #### SQL 的执行过程：
  
  ![image.png](https://upload-images.jianshu.io/upload_images/10402860-500cdb25a0b83b81.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
  
  1. Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree
  2. 遍历AST Tree，抽象出查询的基本组成单元QueryBlock
  3. 遍历QueryBlock，翻译为执行操作树OperatorTree
  4. 逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量
  5. 遍历OperatorTree，翻译为MapReduce任务
  6. 物理层优化器进行MapReduce任务的变换，生成最终的执行计划
  
  
  #### 源码分析:
  
  > ParseDriver:
  
  入参是一条字符串的sql，输出是一棵树（**AST**[抽象语法树（abstract syntax tree或者缩写为AST），或者语法树（syntax tree）])
  
  ASTNode 是树的头结点，他有孩子的数组，
  
  ```
  public ASTNode parse(String command, Context ctx, boolean setTokenRewriteStream) 
      throws ParseException {
    ...
  
    ASTNode tree = (ASTNode) r.getTree();
    tree.setUnknownTokenBoundaries();
    return tree;
  }
  ```
  
  ASTNode获取孩子节点的方法：
  
  ```
  public ArrayList<Node> getChildren() {
    if (super.getChildCount() == 0) {
      return null;
    }
  
    ArrayList<Node> ret_vec = new ArrayList<Node>();
    for (int i = 0; i < super.getChildCount(); ++i) {
      ret_vec.add((Node) super.getChild(i));
    }
  
    return ret_vec;
  }
  ```
  
  > QueryBlock:
  
  接下来是一颗抽象语法树变成一个QB（query block） 
  
  位置　: SemanticAnalyzer.java （语义分析器），之前老的版本大约将近7000行代码，由于Java一个类的代码行数过多时会出现编译上的问题，现在优化分割了。
  
  需要一个树的根节点ast就能对整棵树进行解析（深度优先搜索）。
  ```
  boolean genResolvedParseTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticException {
    ...
    if (ast.getToken().getType() == HiveParser.TOK_CREATETABLE) {
      if ((child = analyzeCreateTable(ast, qb, plannerCtx)) == null) { // 填充 创建表所需要的相关属性， 填充到 rootTask -> work -> createTableDesc 的相关信息中
        return false;
      }
    } else {
      SessionState.get().setCommandType(HiveOperation.QUERY);
    }
  
    // 3. analyze create view command
      if (ast.getToken().getType() == HiveParser.TOK_CREATEVIEW
        || (ast.getToken().getType() == HiveParser.TOK_ALTERVIEW && ast.getChild(1).getType() == HiveParser.TOK_QUERY)) {
      child = analyzeCreateView(ast, qb);
      ....
    preProcessForInsert(child, qb);
    if (!doPhase1(child, qb, ctx_1, plannerCtx)) {
      // 如果doPhase1执行成功那么就会得到一个QB if phase1Result false return
      return false;
    }
    ..
    getMetaData(qb); //从数据库中获得相关信息
    .....
  
    return true;
  }
  ```
  
  QB 对象：
  
  ```
  public class QB {
  
    private static final Log LOG = LogFactory.getLog("hive.ql.parse.QB");
  
    private final int numJoins = 0;
    private final int numGbys = 0;
    private int numSels = 0;
    private int numSelDi = 0;
    private HashMap<String, String> aliasToTabs;  
    private HashMap<String, QBExpr> aliasToSubq;                // 保存子查询的QB对象，aliasToSubq key值是子查询的别名
    private HashMap<String, Map<String, String>> aliasToProps;
    private List<String> aliases;
    private QBParseInfo qbp;                                    // 即QBParseInfo保存一个基本SQL单元中的给个操作部分的AST Tree结构
    private QBMetaData qbm;                                     // 保存每个输入表的元信息，比如表在HDFS上的路径，保存表数据的文件格式等。
    private QBJoinTree qbjoin;
    private String id;
    private boolean isQuery;
    private boolean isAnalyzeRewrite;
    private CreateTableDesc tblDesc = null; // table descriptor of the final
    private CreateTableDesc directoryDesc = null ;
    private List<Path> encryptedTargetTablePaths;
  ```
  
  在analyzeInternal方法中 Operator sinkOp = genPlan(qb); 我们看一下Operator 类的结构：
  
  > Operator Tree:
  
  ```
  public abstract class Operator<T extends Serializable> implements Serializable,
      Node {
  
    // Bean methods
  
    private static final long serialVersionUID = 1L;
  
    protected List<Operator<? extends Serializable>> childOperators;
    protected List<Operator<? extends Serializable>> parentOperators;
    protected String operatorId;
    ......................略.............
  ```
  
  从代码中可以看到Operator 有很多children和parent，由此这是一个有向无环图（DAG）,QB经过genPlan（）方法变成了一个DAG,接下来的Optimizer optm = new Optimizer();
  是逻辑优化器，那么hive有多少逻辑优化器呢？进入Optimizer：
  
  
  
  ```
   public void initialize(HiveConf hiveConf) {
    ...
    transformations.add(new HiveOpConverterPostProc());
  
    // Add the transformation that computes the lineage information.
    transformations.add(new Generator());
    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTPPD)) {
      transformations.add(new PredicateTransitivePropagate());
      if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTCONSTANTPROPAGATION)) {
        transformations.add(new ConstantPropagate());
      }
      transformations.add(new SyntheticJoinPredicate());
      transformations.add(new PredicatePushDown());
    }
    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTCONSTANTPROPAGATION)) {
      // We run constant propagation twice because after predicate pushdown, filter expressions
      // are combined and may become eligible for reduction (like is not null filter).
        transformations.add(new ConstantPropagate());
    }
    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTPPD)) {
      transformations.add(new PartitionPruner());
      transformations.add(new PartitionConditionRemover());
      if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTLISTBUCKETING)) {
        /* Add list bucketing pruner. */
        transformations.add(new ListBucketingPruner());
      }
    }
  
    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTGROUPBY) ||
        HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_MAP_GROUPBY_SORT)) {
      transformations.add(new GroupByOptimizer());
    }
    transformations.add(new ColumnPruner());
    ...
  }
  ```
  以上逻辑是整个hivesql的编译流程代码的大体脉络。
  
  
  #### 程序入口:
  
  - `CliDriver.java`:
  
  ```
  public static void main(String[] args) throws Exception {
    int ret = new CliDriver().run(args);
    System.exit(ret);
  }
  ```
  
  main方法调用了CliDriver实体的run 方法： 
  在run 方法中最后返回的是executeDriver方法
  
  ```
  public  int run(String[] args) throws Exception {
  。。。。。。。。。略 。。。
   return executeDriver(ss, conf, oproc);
  。。。。。。。。。略
  ```
  
  继续跟进executeDriver():
  
  ```
  private int executeDriver(CliSessionState ss, HiveConf conf, OptionsProcessor oproc)
      throws Exception {
  。。。。。。。。略
   while ((line = reader.readLine(curPrompt + "> ")) != null) {
      if (!prefix.equals("")) {
        prefix += '\\n';
      }
      if (line.trim().startsWith("--")) {
        continue;
      }
      if (line.trim().endsWith(";") && !line.trim().endsWith("\\\\;")) {
        line = prefix + line;
        ret = cli.processLine(line, true);
        prefix = "";
        curDB = getFormattedDb(conf, ss);
        curPrompt = prompt + curDB;
        dbSpaces = dbSpaces.length() == curDB.length() ? dbSpaces : spacesForString(curDB);
      } else {
        prefix = prefix + line;
        curPrompt = prompt2 + dbSpaces;
        continue;
      }
    }
    。。。。。。。。。。略
  ```
  executeDriver方法将一条sql用“；”拆分成多条语句，每条语句执行 ret = cli.processLine(line, true);
  
  ```
  /**
   * Processes a line of semicolon separated commands
   * @param line The commands to process
   * @param allowInterrupting  When true the function will handle SIG_INT (Ctrl+C) by interrupting the processing and               
   * returning -1
   * @return 0 if ok
   */
  public int processLine(String line, boolean allowInterrupting) {
  。。。。。。。。。略
   ret = processCmd(command);
   。。。。。。。。。略
  ```
  
  然后进入processCmd方法：
  
  ```
  public int processCmd(String cmd) {
  。。。。。。。。。略。。。。。。。
  if (cmd_trimmed.toLowerCase().equals("quit") || cmd_trimmed.toLowerCase().equals("exit")) {
  。。。。。。。。。略。。。。。。。
  } else if (tokens[0].equalsIgnoreCase("source")) {
  。。。。。。。。。略。。。。。。。
    } else if (cmd_trimmed.startsWith("!")) {
    。。。。。。。。。略。。。。。。。
        }  else { // local mode
              try {
        CommandProcessor proc = CommandProcessorFactory.get(tokens, (HiveConf) conf);
        ret = processLocalCmd(cmd, proc, ss);
      } catch (SQLException e) {
        console.printError("Failed processing command " + tokens[0] + " " + e.getLocalizedMessage(),
          org.apache.hadoop.util.StringUtils.stringifyException(e));
        ret = 1;
      }
    }
    ss.resetThreadName();
    return ret;
  ```
  
  首先processCmd判断是不是退出命令，然后是source和“！”开始的特殊命令（非SQL）的处理，最后是sql的处理逻辑， 
  
  `CommandProcessor proc = CommandProcessorFactory.get(tokens, (HiveConf) conf);`
  
  这句生成了一个CommandProcessor ，那么CommandProcessor 是个什么鬼呢？进入get方法看看：
  
  ```
  public static CommandProcessor get(String[] cmd, HiveConf conf)
        throws SQLException {
      CommandProcessor result = getForHiveCommand(cmd, conf);
      if (result != null) {
        return result;
      }
      if (isBlank(cmd[0])) {
        return null;
      } else {
        if (conf == null) {
          return new Driver();//此处返回的是一个Driver，即Driver是CommandProcessor 的下属类型。
        }
        Driver drv = mapDrivers.get(conf);
        if (drv == null) {
          drv = new Driver();
          mapDrivers.put(conf, drv);
        } else {
          drv.resetQueryState();
        }
        drv.init();
        return drv;
      }
    }
  ```
  
  所以
  
  ```
  CommandProcessor proc = CommandProcessorFactory.get(tokens, (HiveConf) conf);
  ret = processLocalCmd(cmd, proc, ss);
  ```
  
  这里的proc是一个Driver，进入processLocalCmd：
  
  ```
  int processLocalCmd(String cmd, CommandProcessor proc, CliSessionState ss) {
    int tryCount = 0;
    boolean needRetry;
    int ret = 0;
  
    do {
      try {
        needRetry = false;
        if (proc != null) {
          if (proc instanceof Driver) {//一定为true
            Driver qp = (Driver) proc;
            PrintStream out = ss.out;
            long start = System.currentTimeMillis();
            if (ss.getIsVerbose()) {
              out.println(cmd);
            }
  
            qp.setTryCount(tryCount);
            ret = qp.run(cmd).getResponseCode();//此处调用的是Driver的run
            if (ret != 0) {
              qp.close();
              return ret;
            }
            。。。。。。。。。。略
  ```
  
  进入run方法
  
  ```
    @Override
    public CommandProcessorResponse run(String command)
        throws CommandNeedRetryException {
      return run(command, false);
    }
  
  public CommandProcessorResponse run(String command, boolean alreadyCompiled)
          throws CommandNeedRetryException {
      CommandProcessorResponse cpr = runInternal(command, alreadyCompiled);
  
      if(cpr.getResponseCode() == 0) {
        return cpr;
      }
      SessionState ss = SessionState.get();
      if(ss == null) {
        return cpr;
      }
      MetaDataFormatter mdf = MetaDataFormatUtils.getFormatter(ss.getConf());
      if(!(mdf instanceof JsonMetaDataFormatter)) {
        return cpr;
      }
  ```
  
  
  `CommandProcessorResponse cpr = runInternal(command, alreadyCompiled);` 执行sql的编译和返回结果：
  
  
  ```
  private CommandProcessorResponse runInternal(String command, boolean alreadyCompiled)
      throws CommandNeedRetryException {
      。。。。。略
        // compile internal will automatically reset the perf logger
        ret = compileInternal(command, true);
    。。。。。。。略
  ```
  
  compileInternal方法：
  
  ```
  private int compileInternal(String command, boolean deferClose) {
   。。。。。。略。。。。
  
      try {
        ret = compile(command, true, deferClose);
      } finally {
        compileLock.unlock();
      }
  ```
  
  `compile(command, true, deferClose);` 就是hive的入口了。
  
  Driver的run方法最终会执行`compile()`操作, Compiler作语法解析和语义分析。 
  
  
  
  ![](http://img.blog.csdn.net/20170507142034604?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3pxNjU3ODcwMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
  
  
'''
tags: [
  "源码分析"
]
isStarred: false
isTrashed: false
createdAt: "2017-12-25T03:48:02.653Z"
updatedAt: "2018-03-15T11:43:36.178Z"
