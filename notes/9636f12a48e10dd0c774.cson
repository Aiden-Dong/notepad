type: "MARKDOWN_NOTE"
folder: "d218c9600accb5c277d2"
title: "5-spark-application"
content: '''
  ### 5-spark-application
  
  ---
  
  #### 依赖
  
  - scala : `scala-2.11.8`
  
  - spark : `spark-2.1.0-bin-hadoop2.7`
  
  - maven :
  
  ```
  <properties>
      <scala-version>2.11.8</scala-version>
  </properties>
  
  <dependencies>
      <!-- 添加 scala 依赖 -->
      <dependency>
          <groupId>org.scala-lang</groupId>
          <artifactId>scala-library</artifactId>
          <version>${scala-version}</version>
      </dependency>
      <dependency>
          <groupId>org.scala-lang</groupId>
          <artifactId>scala-compiler</artifactId>
          <version>${scala-version}</version>
      </dependency>
      <dependency>
          <groupId>org.scala-lang</groupId>
          <artifactId>scala-reflect</artifactId>
          <version>${scala-version}</version>
      </dependency>
      
      <!-- 添加 spark 依赖 -->
      <dependency>
          <groupId>org.apache.spark</groupId>
          <artifactId>spark-core_2.11</artifactId>
          <version>2.1.0</version>
          <scope>provided</scope>
      </dependency>
  </dependencies>
  ```
  
  #### 1. 初始化 spark 
  
  Spark 编程的第一步是需要创建一个 SparkContext 对象， 用来告诉 Spark 如何访问集群。 
  在创建 SparkContext 之前， 你需要构建一个 SparkConf 对象， SparkConf 对象包含了一些你应用程序的信息。
  
  ```
  val conf = new SparkConf().setAppName(appName).setMaster(master)
  new SparkContext(conf)
  ```
  
  appName 参数是你程序的名字， 它会显示在 cluster UI 上。 master 是 Spark, Mesos 或YARN 集群的 URL， 或运行在本地模式时， 使用专用字符串 “local”。
  在实践中， 当应用程序运行在一个集群上时， 你并不想要把 master 硬编码到你的程序中， 你可以用 spark-submit启动你的应用程序的时候使用 --master传递它。
  然而，你可以在本地测试和单元测试中使用 “local” 运行Spark 进程。
  
  #### 2.读取文件
  
  textFile 方法也可以选择第二个可选参数来控制分区(slices)的数目。 默认情况下，Spark 为每一个文件块(HDFS 默认文件块大小是 64M)创建一个分区(slice)。 但是你也可以通过一个更大的值来设置一个更高的分区数目。 注意， 你不能设置一个小于文件块数目的分区值。
  
  所有 Spark 的基于文件的方法， 包括 textFile，能很好地支持文件目录， 压缩过的文件和通配符。
  例如， 你可以使用 textFile("/my/文件目录"),textFile("/my/文件目录/*.txt") 和 textFile("/my/文件目录/*.gz")
  
  SparkContext.wholeTextFiles 让你读取一个包含多个小文本文件的文件目录并且返回每一个(filename, content)对。 
  与 textFile 的差异是：它记录的是每个文件中的每一行。它的key为文件名称， value 为文件内容。
  
  对于其他的 Hadoop InputFormats， 你可以使用 SparkContext.hadoopRDD方法，它可以指定任意的 JobConf ，输入格式(InputFormat)，key 类型，values 类型。
  你可以跟设置 Hadoop job 一样的方法设置输入源。 你还可以在新的 MapReduce 接口(org.apache.hadoop.mapreduce)基础上使用 SparkContext.newAPIHadoopRDD
  (译者注：老的接口是 SparkContext.newHadoopRDD )。
  
  RDD.saveAsObjectFile 和 SparkContext.objectFile 支持保存一个RDD， 保存格式是一个简单的 Java 对象序列化格式。
  这是一种效率不高的专有格式， 如 Avro， 它提供了简单的方法来保存任何一个 RDD。
  
  ```
  object DerbyCount extends App{
  
    // 创建 spark config
    val sparkConf:SparkConf = new SparkConf().setAppName("derbyContext")
    // 创建 spark context
    val sparkContext:SparkContext = new SparkContext(sparkConf)
  
  
    // 创建rdd 操作
    val derbyRdd = sparkContext.textFile("/data/input/derby.log")
  
    derbyRdd.flatMap(_.split(",")).map((_,1)).reduceByKey(_+_).foreach(data=>{
      if(data._2 > 10){
        println(data._1)
      }
    })
  }
  ```
  
  #### 3.RDD 持久化
  
  Spark最重要的一个功能是它可以通过各种操作（ operations） 持久化（ 或者缓存） 一个集合到内存中。 
  当你持久化一个RDD的时候， 每一个节点都将参与计算的所有分区数据存储到内存中， 并且这些数据可以被这个集合（ 以及这个集合衍生的其他集合） 的动作（ action） 重复利用。
  这个能力使后续的动作速度更快（ 通常快10倍以上） 。 对应迭代算法和快速的交互使用来说， 缓存是一个关键的工具。
  
  你能通过 persist() 或者 cache() 方法持久化一个rdd。 首先， 在action中计算得到rdd；然后， 将其保存在每个节点的内存中。 
  Spark的缓存是一个容错的技术-如果RDD的任何一个分区丢失， 它可以通过原有的转换（ transformations） 操作自动的重复计算并且创建出这个分区。
  
  此外， 我们可以利用不同的存储级别存储每一个被持久化的RDD。 例如， 它允许我们持久化集合到磁盘上、 将集合作为序列化的Java对象持久化到内存中、 在节点间复制集合或者存储集合到 Tachyon中。
  我们可以通过传递一个 StorageLevel 对象给 persist() 方法设置这些存储级别。 
  cache() 方法使用了默认的存储级别— StorageLevel.MEMORY_ONLY 。
  
  ![](http://img.my.csdn.net/uploads/201710/31/1509437108_6202.png)
  
  > 如何选择存储级别
  
  Spark的多个存储级别意味着在内存利用率和cpu利用效率间的不同权衡。 我们推荐通过下面的过程选择一个合适的存储级别：
  ```
  - 如果你的RDD适合默认的存储级别（MEMORY_ONLY)，就选择默认的存储级别。 因为这是cpu利用率最高的选项， 会使RDD上的操作尽可能的快。
  - 如果不适合用默认的级别， 选择MEMORY_ONLY_SER。 选择一个更快的序列化库提高对象的空间使用率， 但是仍能够相当快的访问
  - 除非函数计算RDD的花费较大或者它们需要过滤大量的数据， 不要将RDD存储到磁盘上， 否则， 重复计算一个分区就会和重磁盘上读取数据一样慢
  - 如果你希望更快的错误恢复， 可以利用重复(replicated)存储级别。 所有的存储级别都可以通过重复计算丢失的数据来支持完整的容错， 但是重复的数据能够使你在RDD上继续运行任务， 而不需要重复计算丢失的数据
  - 在拥有大量内存的环境中或者多应用程序的环境中， OFF_HEAP具有如下优势：
     -> 它运行多个执行者共享Tachyon中相同的内存池
     -> 它显著地减少垃圾回收的花费
     -> 如果单个的执行者崩溃， 缓存的数据不会丢失
  ```
  
  #### 4.删除数据 : 
  
  Spark自动的监控每个节点缓存的使用情况， 利用最近最少使用原则删除老旧的数据。 如果你想手动的删除RDD， 可以使用 RDD.unpersist() 方法
  
  #### 5.共享变量
  
  一般情况下， 当一个传递给Spark操作(例如map和reduce)的函数在远程节点上面运行时，Spark操作实际上操作的是这个函数所用变量的一个独立副本。 这些变量被复制到每台机器上， 
  并且这些变量在远程机器上 的所有更新都不会传递回驱动程序。 通常跨任务的读写变量是低效的， 
  但是， Spark还是为两种常见的使用模式提供了两种有限的共享变量：广播变量（broadcast variable） 和累加器（ accumulator）
  
  ##### 5.1 广播变量
  
  广播变量允许程序员缓存一个只读的变量在每台机器上面， 而不是每个任务保存一份拷贝。例如， 利用广播变量， 我们能够以一种更有效率的方式将一个大数据量输入集合的副本分配给每个节点.
  Spark也尝试着利用有效的广播算法去分配广播变量， 以减少通信的成本。
  一个广播变量可以通过调用 SparkContext.broadcast(v) 方法从一个初始变量v中创建。 广播变量是v的一个包装变量， 它的值可以通过 value 方法访问， 下面的代码说明了这个过程：
  
  ```
  scala> val broadcastVar = sc.broadcast(Array(1, 2, 3))
  broadcastVar: spark.Broadcast[Array[Int]] = spark.Broadcast(b5c40191-a864-4c7d-b9bf-d87e1a4e787c)
  scala> broadcastVar.value
  res0: Array[Int] = Array(1, 2, 3)
  ```
  ##### 5.2 累加器
  
  顾名思义， 累加器是一种只能通过关联操作进行“加”操作的变量， 因此它能够高效的应用于并行操作中。 它们能够用来实现 counters 和 sums 。
  Spark原生支持数值类型的累加器， 开发者可以自己添加支持的类型。 如果创建了一个具名的累加器， 它可以在spark的UI中显示。 这对于理解运行阶段(running stages)的过程有很重要的作用。 
  
  一个累加器可以通过调用 SparkContext.accumulator(v) 方法从一个初始变量v中创建。 运行在集群上的任务可以通过 add 方法或者使用 += 操作来给它加值。 
  然而， 它们无法读取这个值。 只有驱动程序可以使用 value 方法来读取累加器的值。 如下的代码， 展示了如何利用累加器将一个数组里面的所有元素相加：
  
  ```
  scala> val accum = sc.longAccumulator("test")
  accum: spark.Accumulator[Int] = 0
  scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)
  ...
  10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s
  scala> accum.value
  res2: Int = 10
  ```
  
  尽管此代码用于Long类型蓄电池的内置支持，程序员还可以通过继承创建自己的类型AccumulatorV2。
  AccumulatorV2抽象类有几种方法必须覆盖：reset用于将累加器重置为零，add用于将另一个值添加到累加器中，merge用于将另一个相同类型的累加器合并到该累加器中。
  必须覆盖的其他方法包含在API文档中。例如，假设我们有一个MyVector表示数学向量的类，我们可以写：
  
  ```
  class VectorAccumulatorV2 extends AccumulatorV2[MyVector, MyVector] {
  
    private val myVector: MyVector = MyVector.createZeroVector
  
    def reset(): Unit = {
      myVector.reset()
    }
  
    def add(v: MyVector): Unit = {
      myVector.add(v)
    }
    ...
  }
  
  // Then, create an Accumulator of this type:
  val myVectorAcc = new VectorAccumulatorV2
  // Then, register it into spark context:
  sc.register(myVectorAcc, "MyVectorAcc1")
  ```
  
  
  #### 5.提交
  
  ```
  spark-submit --class main.DerbyCount \\
  --master yarn \\
  --deploy-mode cluster \\
  --driver-memory 4g \\
  --executor-memory 2g \\
  --executor-cores 1 \\
  --queue thequeue \\
  lib/spark-examples*.jar
  ```
'''
tags: []
isStarred: false
isTrashed: false
createdAt: "2017-10-31T06:51:40.047Z"
updatedAt: "2017-10-31T09:29:15.030Z"
