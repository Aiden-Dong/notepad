type: "MARKDOWN_NOTE"
folder: "fec47565a0577152774f"
title: "6- SVM 支持向量机"
content: '''
  #### 6- SVM 支持向量机
  
  ---
  
  参考自:
  [https://zhuanlan.zhihu.com/p/28660098](https://zhuanlan.zhihu.com/p/28660098)
  [http://www.cnblogs.com/liqizhou/archive/2012/05/11/2495689.html](http://www.cnblogs.com/liqizhou/archive/2012/05/11/2495689.html)
  [https://wizardforcel.gitbooks.io/dm-algo-top10/content/svm-5.html](https://wizardforcel.gitbooks.io/dm-algo-top10/content/svm-5.html)
  [http://backnode.github.io/pages/2015/12/22/SVM-dual.html](http://backnode.github.io/pages/2015/12/22/SVM-dual.html)
  [http://blog.csdn.net/luoshixian099/article/details/51227754](http://blog.csdn.net/luoshixian099/article/details/51227754)
  
  
  欢迎阅读原创。
  
  ### 1. 支持条件
  
  ##### 什么是支持向量机
  
  对于线性可分两类数据，支持向量机就是条直线(对于高维数据点就是一个超平面)。
  两类数据点中的的分割线有无数条，SVM就是这无数条中最完美的一条，怎么样才算最完美呢？
  就是这条线距离两类数据点越远，则当有新的数据点的时候我们使用这条线将其分类的结果也就越可信。
  例如下图中的三条直线都可以将A中的数据分类
  
  > 那条可以有最优的分类能力呢？
  
  1. 我们需要线找到数据点中距离分割超平面距离最近的点(找最小)
  2. 然后尽量使得距离超平面最近的点的距离的绝对值尽量的大(求最大)
  
  ![](https://pic2.zhimg.com/50/v2-35a993aa550d744e45bf51c5db896fd5_hd.jpg)
  
  这里的数据点到超平面的距离就是**间隔**(margin), 当间隔越大，我们这条线(分类器)也就越健壮。
  
  那些距离分割平面最近的点就是支持向量(Support Vectors).
  
  
  一句话总结下就是:
  
  **支持向量机就是用来分割数据点那个分割面.
  他的位置是由支持向量确定的(如果支持向量发生了变化，往往分割面的位置也会随之改变), 
  因此这个面就是一个支持向量确定的分类器即支持向量机。**
  
  ### 2. 构建模型
  
  ##### 求解支持向量机
  
  本部分总结如何获取数据集的最优间隔分割平面(支持向量机)。
  
  ##### 分割超平面
  
  将一维直线和二维平面拓展到任意维, 分割超平面可以表示成:
  
  $$w^{T}x + b = 0$$
  
  其中`w`和`b`就是SVM的参数，不同的`w`和`b`确定不同的分割面.
  
  这里我们可以回忆一下Logistic回归，在Logistic回归模型中，我们也同样将 $$z=w^{T}x$$ 放入到sigmoid函数中来做极大似然估计获取最有的参数 `w` ，其中logistic模型中 `w` 中的 `w_{0}` 便对应着现在我们这里的截距 `b` 。
  
  但是与Logistic回归中我们将 $$z=w^{T}x + b$$ 代入到sigmoid函数中获取的值为1或者0也就是数据标签为0或1。
  
  而在SVM中我们对于二分类，不再使用0/1而是使用+1/-1作为数据类型标签。之所以使用+1/-1是为了能方便的使用间隔公式来表示数据点到分割面的间隔。
  
  
  
  ##### 数据点与超平面的间隔
  
  根据数据点到分割超平面的距离公式:
  
  $$d = \\frac{1}{\\lVert w \\rVert} |w^{T}x + b|$$
  
  可见，在距离公式中有两个绝对值，其中分母上是常量，分子上则是与数据点相关的:
  如果数据点在分割平面上方， $$w^{T}x + b > 0$$ ; 
  数据点在分割平面下方， $$w^{T}x + b < 0$$ 。
  
  这样我们在表示任意数据点到分割面的距离就会很麻烦，但是我们通过将数据标签设为+1/-1来讲距离统一用一个公式表示:
  
  $$d = y_{i} \\cdot (w^{T} + b) \\cdot \\frac{1}{\\lVert w \\rVert}$$
  
  这样，当数据点在分割面上方时， $$y_{i}=1$$ , d > 0 且数据点距离分割面越远 d 越大;
  当数据点在分割面下方时， $$y_{i}=-1$$ ,  d 扔大于0, 且数据点距离分割面越远 d 越大。
  
  ##### 目标函数
  
  我们现在已经有了间隔的公式，我们需要找到一组最好的 w 和 b 确定的分割超平面使得**支持向量**距离此平面的**间隔**最大。
  
  ![](https://pic3.zhimg.com/50/v2-54182cabef9f26a2548976d8615a44d6_hd.jpg)
  
  #####  直接形式
  
  直接使用公式表示:
  
  $$arg \\max \\limits_{w, b} \\{ \\min \\limits_{n} (y_{i} \\cdot (w^{T} + b)) \\cdot \\frac{1}{\\lVert w \\rVert} \\}$$
  
  通俗翻译下就是现在数据点中找到距离分割平面最近的点(支持向量)，然后优化**w**和**b**来最大化支持向量到分割超平面的距离。
  
  直接优化上面的式子很困难，我们需要做一些处理，使得同样的优化问题可以使我们用方便的优化算法来求解。
  
  ##### 等比例改变参数 w 和 b
  
  首先看下分割超平面的一个性质。
  当我们等比例的扩大或缩小ww和bb并不会改变超平面的位置。
  例如对于位于三维空间中的二维平面 $$3x + 2y + z + 5 = 0$$ ,$$w=[3, 2, 1]^{T}$$ , b=5 ，我们等比例扩大或者缩小`w`和`b`并不会影响平面.
  即 $$\\frac{3}{2} + y + \\frac{1}{2}z = 0$$ 与原始平面相同。
  
  这样我们就可以任意等比例修改参数，来使我们优化的目标表达起来更加友好。
  
  > 几何间隔和函数间隔
  
  - 函数间隔(Functional Margin):  $$\\hat{\\gamma_{i}} = y_{i}(wx_{i}^{T} + b)$$
  - 几何间隔(Geometry Margin):  $$\\gamma_{i} = y_{i}(\\frac{wx_{i}^{T}}{\\lVert w \\rVert} + \\frac{b}{\\lVert w \\rVert})$$
  
  可见由于我们可以等比例的改变参数，函数间隔相当于参数都乘上了 $$\\lVert w \\rVert$$ .
  
  ##### 标准形式
  
  我们首先考虑一个决策面是否能够将所有的样本都正确分类的约束。图中的样本点分成两类（红色和蓝色），我们为每个样本点 $${x}_i$$  加上一个类别标签$$y_i$$：
  
  $$y_i = \\left\\{\\begin{array}{ll}+1 & \\textrm{for blue points}\\\\-1 & \\textrm{for red points}\\end{array}\\right.$$ (2.7)
  
  如果我们的决策面方程能够完全正确地对图2中的样本点进行分类，就会满足下面的公式
  
  ![](http://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D+%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%3E0+%26+%5Ctextrm%7Bfor%7E%7E%7D+y_i%3D1%5C%5C%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%3C0+%26+%5Ctextrm%7Bfor%7E%7E%7D+y_i%3D-1%5Cend%7Barray%7D%5Cright.)(2.8)
  
  如果我们要求再高一点，假设决策面正好处于间隔区域的中轴线上，并且相应的支持向量对应的样本点到决策面的距离为d，那么公式(2.8)就可以进一步写成：
  
  ![](http://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D+%28%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%29%2F%7C%7C%5Cboldsymbol%7B%5Comega%7D%7C%7C%5Cgeq+d+%26+%5Cforall%7E+y_i%3D1%5C%5C%28%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%29%2F%7C%7C%5Cboldsymbol%7B%5Comega%7D%7C%7C%5Cleq+-d+%26+%5Cforall%7Ey_i%3D-1%5Cend%7Barray%7D%5Cright.)（2.9）
  
  符号$$\\forall$$是“对于所有满足条件的” 的缩写。我们对公式(2.9)中的两个不等式的左右两边除上d，就可得到：
  
  ![](http://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D+%5Cboldsymbol%7B%5Comega%7D_d%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma_d%5Cgeq+1+%26+%5Ctextrm%7Bfor%7E%7E%7D+y_i%3D1%5C%5C%5Cboldsymbol%7B%5Comega%7D_d%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma_d%5Cleq+-1+%26+%5Ctextrm%7Bfor%7E%7E%7D+y_i%3D-1%5Cend%7Barray%7D%5Cright.) (2.10)
  
  其中
  
  ![](http://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Comega%7D_d+%3D+%5Cfrac%7B%5Cboldsymbol%7B%5Comega%7D%7D%7B%7C%7C%5Cboldsymbol%7B%5Comega%7D%7C%7Cd%7D%2C%7E%7E+%5Cgamma_d+%3D+%5Cfrac%7B%5Cgamma%7D%7B%7C%7C%5Cboldsymbol%7B%5Comega%7D%7C%7Cd%7D)
  
  **写成标准形式**:
  
  ![](http://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D+%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%5Cgeq+1+%26+%5Ctextrm%7Bfor%7E%7E%7D+y_i%3D1%5C%5C%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%5Cleq+-1+%26+%5Ctextrm%7Bfor%7E%7E%7D+y_i%3D-1%5Cend%7Barray%7D%5Cright.) (2.11)
  
  
  
  公式(2.11)里面 ![](http://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%3D+1%7E%7E%5Ctextrm%7Bor%7D%7E%7E-1) 的情况什么时候会发生呢，参考一下公式(2.9)就会知道，只有当 $${x}_i$$ 是 ![](http://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D%2B%5Cgamma%3D0)所对应的支持向量样本点时，等于1或-1的情况才会出现。这一点给了我们另一个简化目标函数的启发。回头看看公式(2.6)，你会发现等式右边分子部分的绝对值符号内部的表达式正好跟公式(2.11)中不等式左边的表达式完全一致，无论原来这些表达式是1或者-1，其绝对值都是1。所以对于这些支持向量样本点有：
  
  ![](http://www.zhihu.com/equation?tex=d+%3D+%5Cfrac%7B%7C%5Cboldsymbol%7B%5Comega%7D%5ET%5Cboldsymbol%7Bx%7D_i%2B%5Cgamma%7C%7D%7B%7C%7C%5Cboldsymbol%7B%5Comega%7D%7C%7C%7D%3D%5Cfrac%7B1%7D%7B%7C%7C%5Cboldsymbol%7B%5Comega%7D%7C%7C%7D%2C%7E%7E%5Ctextrm%7Bif%7D+%7E%5Cboldsymbol%7Bx%7D_i+%5Ctextrm+%7Bis+a+support+vector%7D+) (2.12)
  
  于是有:
  
  $$arg \\max \\limits_{w, b} \\frac{1}{\\lVert w \\rVert}$$ 约束条件: $$y_{i} \\cdot (wx_{i}^{T} + b) \\ge 1$$, i = 1,2,…,k
  
  > 将最大化问题转换为求最小值:
  
  $$arg \\min \\limits_{w, b} \\frac{1}{2} \\lVert w \\rVert ^{2}$$ 约束条件: $$y_{i} \\cdot (wx_{i}^{T} + b) \\ge 1$$, i = 1,2,…,k  ----- (2.13)
  
  这便是一个线性不等式约束下的二次优化问题, 下面我本就使用拉格朗日乘子法来获取我们优化目标的对偶形式。
  
  
  #### 凸二次规划问题
  
  添加了等式限制条件，优化函数为: $$\\min \\limits_{x} f(x)$$ 约束条件 $$h(x) = 0$$
  
  拉格朗日乘子法就是通过引入新的位置变量(拉格朗日橙子)将上式的约束条件一起放到目标函数中:
  
  $$L(x, \\lambda) = f(x) + \\lambda h(x)$$, 限制条件 $$h(x) = 0$$
  
  > 通过求解方程组: $$\\nabla L(x, \\lambda) = 0$$; $$h(x) = 0$$ 便可得到局部最小值的必要条件。
  
  $$arg \\min \\limits_{w, b} \\frac{1}{2} \\lVert w \\rVert ^{2}$$ 限制条件 $$y_{i} \\cdot (wx_{i}^{T} + b) -1 \\ge 0$$, $$i = 1,2,...,k$$
  
  > 对应的拉格朗日函数:
  
  $$L(w, b, \\alpha) = \\frac{1}{2} \\lVert w \\rVert^{2} + \\sum \\limits_{i=1}^{N} \\alpha_{i}[1-y_{i}(w^{T}x_{i} + b)]$$ ------- (2.14)
  
  限制条件 : $$1-y_{i}(w^{T}x_{i} + b) \\le 0$$
  
  其中,  $$\\alpha_{i} \\ge 0$$
  
  令 $$g_{i}(w, b) = 1-y_{i}(w^{T}x_{i} + b)$$，则有 $$g_{i}(w, b) \\le 0$$ .
  
  
  可见，**支持向量对应的约束为活动约束, 我们的目标函数是由支持向量决定的**，毕竟我们就是支持向量机嘛.
  
  ### 3. 对偶问题
  
  $$L(w, b, \\alpha) = \\frac{1}{2} \\lVert w \\rVert^{2} + \\sum \\limits_{i=1}^{N} \\alpha_{i}[1-y_{i}(w^{T}x_{i} + b)]$$ ------- (2.14)
  
  限制条件 : $$1-y_{i}(w^{T}x_{i} + b) \\le 0$$
  
  $$\\frac{\\partial L(w,b, \\alpha)}{\\partial w} = w - \\sum \\limits_{i=1}^{N} \\alpha_{i}y{i}x_{i} = 0$$   ==> $$w=\\sum \\limits_{i=1}^{N} \\alpha_{i}y{i}x_{i}$$  ------ (2.15)
  
  $$\\frac{\\partial L(w,b, \\alpha)}{\\partial b}=\\sum \\limits_{i=1}^{N} \\alpha_{i}y{i} = 0$$  ----- (2.16)
  
  
  将 (2.16) 带入到  (2.14) 式子， 去掉 b 项:
  
  $$L(w, \\alpha) = \\frac{1}{2} \\lVert w \\rVert^{2} - \\sum \\limits_{i=1}^{N} \\alpha_{i}[1-y_{i}(w^{T}x_{i})]$$ ------ （2.17）
  
  然后将 (2.15) 带入到 (2.17) 式子, 得到:
  
  $$L(w, \\alpha) = \\frac{1}{2} w^{T}w + \\sum \\limits_{i=1}^{N} \\alpha_{i}-w^{T}w = \\sum \\limits_{i=1}^{N} \\alpha_{i} - \\frac{1}{2} w^{T}w$$ ----- (2.18)
  
  将公式 (2.15) 带入到 (2.18) 得到:
  
  
  $$L(\\alpha, x, y) = \\sum \\limits_{i=1}^{N} \\alpha_{i} - \\frac{1}{2}(\\sum \\limits_{i=1}^{N} \\sum \\limits_{j=1}^{N}  \\alpha_{i} \\alpha_{j} y{i} y{j} x_{i} \\cdot x_{j})$$  ------ （2.19）
  
  约束条件 : 
  
  $$\\alpha \\ge 0$$
  
  $$\\sum \\limits_{i=1}^{N} \\alpha_{i}y{i} = 0$$
  
  计算`w`:
  
  $$w = \\sum \\limits_{n=1}^{N}\\alpha_n y_n x_n$$
  
  
  #### 4. SMO 算法详解
  
  ##### 4.1 视为一个二元函数 
  
  为了求解N个参数 $$(\\alpha_1,\\alpha_2,\\alpha_3,...,\\alpha_N)$$，首先想到的是坐标上升的思路，例如求解 $$\\alpha_1$$ ,可以固定其他 $$N-1$$ 个参数，可以看成关于$$\\alpha_1$$的一元函数求解，但是注意到上述问题的等式约束条件 $$\\sum \\limits_{i=1}^{N} \\alpha_{i}y{i} = 0$$,当固定其他参数时，参数$$\\alpha_1$$也被固定，因此此种方法不可用。
  
  SMO算法选择同时优化两个参数，固定其他$$N-2$$个参数，假设选择的变量为$$\\alpha_1,\\alpha_2$$,固定其他参数$$\\alpha_3,\\alpha_4,...,\\alpha_N$$,由于参数$$\\alpha_3,\\alpha_4,...,\\alpha_N$$的固定，可以简化目标函数为只关于$$\\alpha_1,\\alpha_2$$的二元函数，Constant表示常数项。
  
  $$min \\Phi(\\alpha_1, \\alpha_2) = ( \\alpha_{1} + \\alpha_{2} ) - \\frac{1}{2} K_{11} \\alpha_{1}^{2} - \\frac{1}{2} K_{22} \\alpha_{2}^{2} - y_{1}y_{2} K_{12} \\alpha_{1} \\alpha_{2}  - y_{1} v_{1} \\alpha_{1} - y_{2} v_{2} \\alpha_{2} + C$$   ------ (2.20)
  
  其中$$v_{i}=\\sum \\limits_{j=3}^{N} \\alpha_{j} y_{j} K(x_{i},x_{j}),i=1,2$$
  
  ##### 4.2 视为一元函数:
  
  由等式约束得：$$\\alpha_{1} y_{1} + \\alpha_{2} y_{2} = - \\sum \\limits_{i=3}^{N} \\alpha_{i} y_{i} = \\zeta$$，可见 $$\\zeta$$ 为定值。
  
  等式 $$\\alpha_{1} y_{1} + \\alpha_{2} y_{2} = \\zeta$$ 两边同时乘以 $$y_{1}$$,且 $$y_{1}^{2} = 1$$ ，得
  
  $$\\alpha_{1} = ( \\zeta - y_{2}\\alpha_{2})y_{1}$$---- (2.21)
  
  上式带回到(2.20)中得到只关于参数 $$\\alpha_{2}$$ 的一元函数，由于常数项不影响目标函数的解，以下省略掉常数项$$C$$
  
  $$min \\Phi( \\alpha_{2} ) = ( \\zeta - \\alpha_{2} y_{2} ) y_{1} + \\alpha_{2} - \\frac{1}{2} K_{11} ( \\zeta - \\alpha_{2} y_{2} )^{2} - \\frac{1}{2} K_{22} \\alpha^{2}_{2} - y_2 K{12} ( \\zeta - \\alpha_{2} y_{2} ) \\alpha_{2} - v_{1} ( \\zeta - \\alpha_{2} y_{2} ) - y_{2} v_{2} \\alpha_{2}$$ 
  
  
  ##### 4.3 对一元函数求极值点
  
  上式中是关于变量 $$\\alpha_{2}$$ 的函数，对上式求导并令其为 $$0$$ 得： 
  
  $$\\frac {\\partial \\Phi(\\alpha_{2})}{\\partial \\alpha_{2}} = - (K_{11} + K_{22} -  2K_{12}) \\alpha_{2} + K_{11} \\zeta y_{2} - K_{12} \\zeta y_{2} - y_{1}y_{2} + 1 + v_{1} y_{2} - v_{2} y_{2}
   = 0$$ 
   
  1.由上式中假设求得了 $$\\alpha_{2}$$ 的解，带回到 (2.21) 式中可求得 $$\\alpha_{1}$$ 的解，分别记为 $$\\alpha_{1}^{new}$$ , $$\\alpha_{2}^{new}$$, 优化前的解记为 $$\\alpha_{1}^{old}$$,$$\\alpha_{2}^{old}$$;由于参数$$\\alpha_{3}$$, $$\\alpha_{4}$$ ,..., $$\\alpha_{N}$$ 固定，由等式约束 $$\\sum \\limits_{i=1}^{N} y_{i} \\alpha_{i} = 0$$ 有 $$\\alpha_{1}^{old} y_{1} +  \\alpha_{2}^{old} y_{2} = - \\sum \\limits_{i=3}^{N} \\alpha_{i} y_{i} = \\alpha_{1}^{new} y_{1} + \\alpha_{2}^{new} y_{2} =  \\zeta$$
  
  $$\\zeta = \\alpha_{1}^{old}y_{1} + \\alpha_{2}^{old}y_{2}$$ ------ (2.22)
  
  2.假设SVM超平面的模型为 $$f(x)=w^{T}x+b$$ , 上一篇中已推导出 $$w$$ 的表达式, 将其带入得 $$f(x)= \\sum \\limits_{i=1}^{N} \\alpha_{i} y_{i} K(xi,x)+b$$; $$f(x_{i})$$ 表示样本 $$x_{i}$$ 的预测值, $$y_{i}$$ 表示样本 $$x_{i}$$ 的真实值，定义 $$E_{i}$$ 表示预测值与真实值之差为
  
  $$E_{i} = f(x_{i}) - y_{i}$$ ------ (2.23)
  
  3.由于 $$v_{i} = \\sum \\limits_{j=3}^{N} \\alpha_{j} y_{j} K(x_{i},x_{j}),i=1,2$$，因此 :
  
  $$v_{1} = f(x_{1}) - \\sum \\limits_{j=1}^{2} y_{j} \\alpha_{j} K_{1j} - b$$ ------ (2.24)
  
  $$v_{2} = f(x_{2}) - \\sum \\limits_{j=1}^{2} y_{j} \\alpha_{j} K_{2j} - b$$  ----- (2.25)
  
  把(2.22), (2.24), (2.25) 带入到 (2.21)中去。　 
  
  化简得: 此时求解出的 $$\\alpha_{2}^{new}$$未考虑约束问题，先记为 $$\\alpha_2^{new,unclipped}$$
  
  $$(K_{11} + K_{22} - 2K_{12}) \\alpha_{2}^{new,unclipped} = (K_{11} + K_{22} - 2K_{12}) \\alpha_2^{old} + y_2[y_2 - y_1 + f(x_1) - f(x_2)]$$
  
  带入(2.23)式，并记 $$\\eta=K_{11} + K_{22} - 2K_{12}$$ 得:
  
  $$\\alpha_{2}^{new,unclipped} = \\alpha^{old}_{2} + \\frac{y_{2}(E_{1} - E_{2})}{\\eta}$$
  
  #### 4.4 对原始值解修剪:
  
  上述求出的解未考虑到约束条件：
  
  $$0 \\le \\alpha_{i=1,2} \\le C$$
  
  $$\\alpha_{1}y_{1} + \\alpha_{2}y_{2} = \\zeta$$
  
  在二维平面上直观表达上述两个约束条件:
  
  ![http://img.blog.csdn.net/20160427143321581](http://img.blog.csdn.net/20160427143321581)
  
  最优解必须要在方框内且在直线上取得，因此 $$L \\ge \\alpha_2^{new} \\ge H$$;
  
  当$$y_{1} \\ne y_{2}$$时, $$L=\\max(0,\\alpha_{2}^{old} - \\alpha_{1}^{old});H=min(C,C + \\alpha_{2}^{old} - \\alpha_{1}^{old})$$ 
  
  当$$y_{1}=y_{2}$$时，$$L=max(0,\\alpha_{1}^{old} + \\alpha_2^{old} - C);H=min(C,\\alpha_{2}^{old} + \\alpha_{1}^{old})$$ 
  
  经过上述约束的修剪，最优解就可以记为$$\\alpha_{2}^{new}$$了。
  
  所以有:
  
  $$\\alpha_{2}^{new} = \\left\\{\\begin{array}{ll} H, \\alpha_{2}^{new, uncliped} \\gt H \\\\  \\alpha_{2}^{new, uncliped}, L \\le \\alpha_{2}^{new, uncliped} \\le H \\\\ L, \\alpha_2^{new, unclipped} \\lt  L \\end{array}\\right.$$  ------ （2.26）
  
  #### 4.5 求解 $$\\alpha_{1}^{new}$$
  
  由于其他 $$N-2$$ 个变量固定，因此 $$\\alpha_{1}^{old}y_{1} + \\alpha_2^{old}y_{2} = \\alpha_{1}^{new}y_{1} + \\alpha_{2}^{new}y_{2}$$ 所以可求得:
  
  $$\\alpha_{1}^{new} = \\alpha_{1}^{old} + y_{1}y_{2} (\\alpha_{2}^{old} - \\alpha_{2}^{new})$$ ------ (2.27)
  
  #### 4.6 取临界情况
  
  大部分情况下，有 $$\\eta = K_{11} + K_{22} - 2K_{12} \\gt 0$$ 。但是在如下几种情况下，$$\\alpha_{2}^{new}$$需要取临界值$$L$$或者$$H$$.
  
  
  1. $$\\eta \\lt 0$$,当核函数K不满足Mercer定理时，矩阵K非正定;
  2. $$\\eta=0$$,样本x1与x2输入特征相同;
  
  $$min \\Phi( \\alpha_{2} ) = \\frac{1}{2} K_{11} ( \\zeta - \\alpha_{2} y_{2} )^{2} + \\frac{1}{2} K_{22} \\alpha^{2}_{2} + y_2 K{12} ( \\zeta - \\alpha_{2} y_{2} ) \\alpha_{2} - ( \\zeta - \\alpha_{2} y_{2} ) y_{1} - \\alpha{2} + v_{1} ( \\zeta - \\alpha_{2} y_{2} ) + y_{2} v_{2} \\alpha_{2}$$ 
  
  也可以如下理解，对上式求二阶导数就是　$$\\eta = K_{11} + K_{22} - 2K_{12}$$, 
  
  当 $$\\eta \\lt 0$$ 时，目标函数为凸函数，没有极小值，极值在定义域边界处取得。 
  当 $$\\eta = 0$$ 时，目标函数为单调函数，同样在边界处取极值。 
  
  > 计算方法： 
  
  即当 $$\\alpha_{2}^{new}=L$$ 和 $$\\alpha_{2}^{new}=H$$ 分别带入(2.27)式中，计算出 $$\\alpha_{1}^{new} = L_{1}$$ 和 $$\\alpha_{1}^{new} = H_{1}$$.
  
  $$L_{1} = \\alpha_{1} + y_1y_2(\\alpha_{2} - L)$$,
  $$H_{1} = \\alpha_{1} + y_1y_2(\\alpha_{2} - H)$$,
  
  带入目标函数(2.20)内，比较$$\\Phi(\\alpha_{1} = L_{1},\\alpha_{2}=L)$$与$$\\Phi(\\alpha_{1} = H_{1},\\alpha_{2}=H)$$的大小，$$\\alpha_{2}$$取较小的函数值对应的边界点。
  
  ![](http://img.blog.csdn.net/20160427164937605)
  
  其中
  
  ![](http://img.blog.csdn.net/20160427170049944)
  
  #### 4.7 启发式选择变量
  
  上述分析是在从N个变量中已经选出两个变量进行优化的方法，下面分析如何高效地选择两个变量进行优化，使得目标函数下降的最快。
  
  ##### 第一个变量的选择:
  
  第一个变量的选择称为外循环，首先遍历整个样本集，选择违反KKT条件的 $$\\alpha_{i}$$作为第一个变量.
  
  接着依据相关规则选择第二个变量(见下面分析),对这两个变量采用上述方法进行优化。
  
  当遍历完整个样本集后，遍历非边界样本集$$(0<\\alpha_{i}<C)$$中违反KKT的$$\\alpha_{i}$$作为第一个变量，同样依据相关规则选择第二个变量，对此两个变量进行优化。
  
  当遍历完非边界样本集后，再次回到遍历整个样本集中寻找，即在整个样本集与非边界样本集上来回切换，寻找违反KKT条件的 $$\\alpha_{i}$$作为第一个变量。
  
  直到遍历整个样本集后，没有违反KKT条件$$\\alpha_{i}$$，然后退出。
  
  边界上的样本对应的$$\\alpha_{i}=0$$或者 $$\\alpha_{i}=C$$，在优化过程中很难变化，然而非边界样本0<αi<C会随着对其他变量的优化会有大的变化。
  
  ![](http://img.blog.csdn.net/20160427212013553)
  
  ##### 第二个变量的选择:
  
  SMO称第二个变量的选择过程为内循环，假设在外循环中找个第一个变量记为 $$\\alpha_{1}$$，第二个变量的选择希望能使$$\\alpha_{2}$$ 有较大的变化.
  
  由于$$\\alpha_{2}$$是依赖于$$|E_{1}-E_{2}|$$
  
  当 $$E_{1}$$ 为正时，那么选择最小的$$E_{i}$$作为$$E_{2}$$.
  
  如果$$E_{1}$$为负，选择最大$$E_{i}$$作为$$E_{2}$$，通常为每个样本的Ei保存在一个列表中，选择最大的|E1−E2|来近似最大化步长。 
  
  有时按照上述的启发式选择第二个变量，不能够使得函数值有足够的下降，这时按下述步骤:
  
  ```
  首先在非边界集上选择能够使函数值足够下降的样本作为第二个变量， 
  如果非边界集上没有，则在整个样本集上选择第二个变量， 
  如果整个样本集依然不存在，则重新选择第一个变量。
  ```
  
  #### 4.7 阈值b的计算
  
  每完成对两个变量的优化后，要$$b$$的值进行更新，因为$$b$$的值关系到$$f(x)$$的计算，即关系到下次优化时$$E_{i}$$的计算。 
  
  1.如果 $$0  \\lt \\alpha_{1}^{new} \\lt C$$, 由KKT条件 $$y1(wTx1+b)=1$$ ,得到 $$\\sum \\limits_{i=1}^{N} \\alpha_{i} y_{i} K_{i1} + b = y_{1}$$,由此得:
  
  $$b_{1}^{new} = y_{1} - \\sum \\limits_{i=3}^{N} \\alpha_{i} y_{i} K_{i1}  - \\alpha_{1}^{new} y_{1}K_{11} - \\alpha_{2}^{new} y_{2} K_{21}$$
  
  由(2.23)式得, 上式前两张可以替换为:
  
  $$y_1 - \\sum \\limits_{i=3}^{N} \\alpha_{i}y_{i}K_{i1} = - E_{1} + \\alpha_{1}^{old} y_{1} k_{11} + \\alpha_{2}^{old} y_{1} K_{11} + \\alpha_{2}^{old}y_{2} k_{11} + b^{old}$$
  
  得出:
  
  $$b_{1}^{new} = - E_{1} - y_{1} K_{11}(\\alpha_{1}^{new} - \\alpha_{1}^{old})- y_{2}K_{21}(\\alpha_{2}^{new} - \\alpha_{2}^{old}) + b^{old}$$
  
  2.如果$$0 \\lt \\alpha_{2}^{new} \\lt C$$，则
  
  $$b_2^{new} = - E_{2} - y_{1}K_{12} (\\alpha_{1}^{new} - \\alpha_{1}^{old}) - y_{2}K_{22} (\\alpha_{2}^{new} - \\alpha_{2}^{old}) + b^{old}$$
  
  3.如果同时满足$$0 \\lt \\alpha_{i}^{new} \\lt C$$,则$$b_{1}^{new} = b_{2}^{new}$$
  
  4.如果同时不满足 $$0 \\lt \\alpha_{i}^{ne} \\lt C$$，则 $$b_{1}^{new}$$ 与 $$b_{2}^{new}$$ 以及它们之间的数都满足KKT阈值条件，这时选择它们的中点。
  
  
'''
tags: []
isStarred: false
isTrashed: false
createdAt: "2017-12-26T09:56:10.572Z"
updatedAt: "2017-12-27T03:55:42.025Z"
