type: "MARKDOWN_NOTE"
folder: "7671e01e5a4a4ee04aa9"
title: "Yarn篇 (2) 调度器Scheduler详解"
content: '''
  ### Yarn篇 (2) 调度器Scheduler详解
  
  转载 : http://blog.csdn.net/suifeng3051/article/details/49508261
  
  ---
  
  理想情况下，我们应用对Yarn资源的请求应该立刻得到满足，但现实情况资源往往是有限的，特别是在一个很繁忙的集群，一个应用资源的请求经常需要等待一段时间才能的到相应的资源。在Yarn中，负责给应用分配资源的就是Scheduler。其实调度本身就是一个难题，很难找到一个完美的策略可以解决所有的应用场景。为此，Yarn提供了多种调度器和可配置的策略供我们选择。
  
  #### 1. 调度器的选择：
  
  在Yarn中有三种调度器可以选择：`FIFO Scheduler` ，`Capacity Scheduler`，`Fair cheduler`。
  
  **FIFO Scheduler**把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。
  
  **FIFO Scheduler**是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。在共享集群中，更适合采用Capacity Scheduler或Fair Scheduler，这两个调度器都允许大任务和小任务在提交的同时获得一定的系统资源。
  
  
  下面“Yarn调度器对比图”展示了这几个调度器的区别，从图中可以看出，在FIFO 调度器中，小任务会被大任务阻塞。
  
  而对于**Capacity**调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。
  
  在**Fair调度器**中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。如下图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。
  
  需要注意的是，在下图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成。
  
  
  ![](http://img.blog.csdn.net/20151030111100329)
  
  ### 2.Capacity Scheduler（容器调度器）的配置
  
  #### 2.1 容器调度介绍
  
  Capacity 调度器允许多个组织共享整个集群，每个组织可以获得集群的一部分计算能力。通过为每个组织分配专门的队列，然后再为每个队列分配一定的集群资源，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。除此之外，队列内部又可以垂直划分，这样一个组织内部的多个成员就可以共享这个队列资源了，在一个队列内部，资源的调度是采用的是先进先出(FIFO)策略。
  
  通过上面那幅图，我们已经知道一个job可能使用不了整个队列的资源。然而如果这个队列中运行多个job，如果这个队列的资源够用，那么就分配给这些job，如果这个队列的资源不够用了呢？其实Capacity调度器仍可能分配额外的资源给这个队列，这就是“弹性队列”(queue elasticity)的概念。
  
  在正常的操作中，Capacity调度器不会强制释放Container，当一个队列资源不够用时，这个队列只能获得其它队列释放后的Container资源。当然，我们可以为队列设置一个最大资源使用量，以免这个队列过多的占用空闲资源，导致其它队列无法使用这些空闲资源，这就是”弹性队列”需要权衡的地方。
  
  #### 2.2 容器调度的配置
  
  假如我们有如下的配置 :
  
  ```
  root
  ├── prod
  └── dev
      ├── eng
      └── science
  ```
  
  下面是一个简单的Capacity调度器的配置文件，文件名为capacity-scheduler.xml。在这个配置中，在root队列下面定义了两个子队列prod和dev，分别占40%和60%的容量。需要注意，一个队列的配置是通过属性yarn.sheduler.capacity.<queue-path>.<sub-property>指定的，<queue-path>代表的是队列的继承树，如root.prod队列，<sub-property>一般指capacity和maximum-capacity。
  
  ![](http://img.blog.csdn.net/20151030111921481)
  
  我们可以看到，dev队列又被分成了eng和science两个相同容量的子队列。dev的maximum-capacity属性被设置成了75%，所以即使prod队列完全空闲dev也不会占用全部集群资源，也就是说，prod队列仍有25%的可用资源用来应急。我们注意到，eng和science两个队列没有设置maximum-capacity属性，也就是说eng或science队列中的job可能会用到整个dev队列的所有资源（最多为集群的75%）。而类似的，prod由于没有设置maximum-capacity属性，它有可能会占用集群全部资源。
  
  Capacity容器除了可以配置队列及其容量外，我们还可以配置一个用户或应用可以分配的最大资源数量、可以同时运行多少应用、队列的ACL认证等。
  
  #### 2.3 队列的设置
  
  关于队列的设置，这取决于我们具体的应用。比如，在MapReduce中，我们可以通过mapreduce.job.queuename属性指定要用的队列。如果队列不存在，我们在提交任务时就会收到错误。如果我们没有定义任何队列，所有的应用将会放在一个default队列中。
  
  注意：对于Capacity调度器，我们的队列名必须是队列树中的最后一部分，如果我们使用队列树则不会被识别。比如，在上面配置中，我们使用prod和eng作为队列名是可以的，但是如果我们用root.dev.eng或者dev.eng是无效的。
  
  
  #### 2.4 容器的使用案例
  
  ```
  <!--
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
  
    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License. See accompanying LICENSE file.
  -->
  <configuration>
  
    <property>
      <name>yarn.scheduler.capacity.maximum-applications</name>
      <value>10000</value>
      <description>
        Maximum number of applications that can be pending and running.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
      <value>0.1</value>
      <description>
        Maximum percent of resources in the cluster which can be used to run
        application masters i.e. controls number of concurrent running
        applications.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.resource-calculator</name>
      <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>
      <description>
        The ResourceCalculator implementation to be used to compare
        Resources in the scheduler.
        The default i.e. DefaultResourceCalculator only uses Memory while
        DominantResourceCalculator uses dominant-resource to compare
        multi-dimensional resources such as Memory, CPU etc.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.queues</name>
      <value>default,saligia,eva</value>
      <description>
        The queues at the this level (root is the root queue).
      </description>
    </property>
  
    <!-- default queue -->
    <property>
      <name>yarn.scheduler.capacity.root.default.capacity</name>
      <value>50</value>
      <description>Default queue target capacity.</description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>
      <value>1</value>
      <description>
        Default queue user limit a percentage from 0.0 to 1.0.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
      <value>100</value>
      <description>
        The maximum capacity of the default queue.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.default.state</name>
      <value>RUNNING</value>
      <description>
        The state of the default queue. State can be one of RUNNING or STOPPED.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>
      <value>*</value>
      <description>
        The ACL of who can submit jobs to the default queue.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>
      <value>*</value>
      <description>
        The ACL of who can administer jobs on the default queue.
      </description>
    </property>
  
    <!-- Saligia -->
    <property>
      <name>yarn.scheduler.capacity.root.saligia.capacity</name>
      <value>30</value>
      <description>Default queue target capacity.</description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.saligia.user-limit-factor</name>
      <value>1</value>
      <description>
        Default queue user limit a percentage from 0.0 to 1.0.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.saligia.maximum-capacity</name>
      <value>100</value>
      <description>
        The maximum capacity of the default queue.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.saligia.state</name>
      <value>RUNNING</value>
      <description>
        The state of the default queue. State can be one of RUNNING or STOPPED.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.saligia.acl_submit_applications</name>
      <value>*</value>
      <description>
        The ACL of who can submit jobs to the default queue.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.saligia.acl_administer_queue</name>
      <value>*</value>
      <description>
        The ACL of who can administer jobs on the default queue.
      </description>
    </property>
  
  
    <!-- eva -->
    <property>
      <name>yarn.scheduler.capacity.root.eva.capacity</name>
      <value>20</value>
      <description>Default queue target capacity.</description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.eva.user-limit-factor</name>
      <value>1</value>
      <description>
        Default queue user limit a percentage from 0.0 to 1.0.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.eva.maximum-capacity</name>
      <value>100</value>
      <description>
        The maximum capacity of the default queue.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.eva.state</name>
      <value>RUNNING</value>
      <description>
        The state of the default queue. State can be one of RUNNING or STOPPED.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.eva.acl_submit_applications</name>
      <value>*</value>
      <description>
        The ACL of who can submit jobs to the default queue.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.root.eva.acl_administer_queue</name>
      <value>*</value>
      <description>
        The ACL of who can administer jobs on the default queue.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.node-locality-delay</name>
      <value>40</value>
      <description>
        Number of missed scheduling opportunities after which the CapacityScheduler
        attempts to schedule rack-local containers.
        Typically this should be set to number of nodes in the cluster, By default is setting
        approximately number of nodes in one rack which is 40.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.queue-mappings</name>
      <value></value>
      <description>
        A list of mappings that will be used to assign jobs to queues
        The syntax for this list is [u|g]:[name]:[queue_name][,next mapping]*
        Typically this list will be used to map users to queues,
        for example, u:%user:%user maps all users to queues with the same name
        as the user.
      </description>
    </property>
  
    <property>
      <name>yarn.scheduler.capacity.queue-mappings-override.enable</name>
      <value>false</value>
      <description>
        If a queue mapping is present, will it override the value specified
        by the user? This can be used by administrators to place jobs in queues
        that are different than the one specified by the user.
        The default is false.
      </description>
    </property>
  
  </configuration>
  
  ```
  
  
  ### 3. Fair Scheduler（公平调度器）的配置
  
  #### 3.1 公平调度
  
  Fair调度器的设计目标是为所有的应用分配公平的资源（对公平的定义可以通过参数来设置）。在上面的“Yarn调度器对比图”展示了一个队列中两个应用的公平调度；当然，公平调度在也可以在多个队列间工作。举个例子，假设有两个用户A和B，他们分别拥有一个队列。当A启动一个job而B没有任务时，A会获得全部集群资源；当B启动一个job后，A的job会继续运行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个job并且其它job还在运行，则它将会和B的第一个job共享B这个队列的资源，也就是B的两个job会用于四分之一的集群资源，而A的job仍然用于集群一半的资源，结果就是资源最终在两个用户之间平等的共享。过程如下图所示： 
  
  ![](http://img.blog.csdn.net/20151030112204338)
  
  #### 3.2 启用Fair Schedule
  
  调度器的使用是通过`yarn-site.xml`配置文件中的`yarn.resourcemanager.scheduler.class`参数进行配置的，默认采用Capacity Scheduler调度器。如果我们要使用Fair调度器，需要在这个参数上配置FairScheduler类的全限定名： `org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler`。
  
  #### 3.3 队列的配置
  
  Fair调度器的配置文件位于类路径下的`fair-scheduler.xml`文件中，这个路径可以通过`yarn.scheduler.fair.allocation.file`属性进行修改。若没有这个配置文件，Fair调度器采用的分配策略，这个策略和3.1节介绍的类似：调度器会在用户提交第一个应用时为其自动创建一个队列，队列的名字就是用户名，所有的应用都会被分配到相应的用户队列中。
  
  
  
  #### 3.4 队列介绍（Queues）
  
  队列是YARN调度程序的组织结构，允许多个租户共享集群。当应用程序提交到YARN时，它们由调度程序分配到队列。根（root）队列是所有队列的父级。所有其他队列都是根队列或另一个队列（也称为分层队列）的子代。队列通常对应于用户，部门或优先级。
  
  
  
  
  
  
  队列的层次是通过嵌套`<queue>`元素实现的。所有的队列都是root队列的孩子，即使我们没有配到`<root>`元素里。在这个配置中，我们把dev队列有分成了eng和science两个队列。
  
  Fair调度器中的队列有一个权重属性（这个权重就是对公平的定义），并把这个属性作为公平调度的依据。在这个例子中，当调度器分配集群40:60资源给prod和dev时便视作公平，eng和science队列没有定义权重，则会被平均分配。这里的权重并不是百分比，我们把上面的40和60分别替换成2和3，效果也是一样的。注意，对于在没有配置文件时按用户自动创建的队列，它们仍有权重并且权重值为1。
  
  每个队列内部仍可以有不同的调度策略。队列的默认调度策略可以通过顶级元素`<defaultQueueSchedulingPolicy>`进行配置，如果没有配置，默认采用公平调度。
  
  尽管是Fair调度器，其仍支持在队列级别进行FIFO调度。每个队列的调度策略可以被其内部的`<schedulingPolicy>` 元素覆盖，在上面这个例子中，prod队列就被指定采用FIFO进行调度，所以，对于提交到prod队列的任务就可以按照FIFO规则顺序的执行了。需要注意，prod和dev之间的调度仍然是公平调度，同样eng和science也是公平调度。
  
  尽管上面的配置中没有展示，每个队列仍可配置最大、最小资源占用数和最大可运行的应用的数量。
  
  air调度器采用了一套基于规则的系统来确定应用应该放到哪个队列。在上面的例子中，<queuePlacementPolicy> 元素定义了一个规则列表，其中的每个规则会被逐个尝试直到匹配成功。例如，上例第一个规则specified，则会把应用放到它指定的队列中，若这个应用没有指定队列名或队列名不存在，则说明不匹配这个规则，然后尝试下一个规则。primaryGroup规则会尝试把应用放在以用户所在的Unix组名命名的队列中，如果没有这个队列，不创建队列转而尝试下一个规则。当前面所有规则不满足时，则触发default规则，把应用放在dev.eng队列中。
  
  当然，我们可以不配置queuePlacementPolicy规则，调度器则默认采用如下规则：
  
  ```
  <queuePlacementPolicy>
  <rule name="specified" />
  <rule name="user" />
  </queuePlacementPolicy>
  ```
  上面规则可以归结成一句话，除非队列被准确的定义，否则会以用户名为队列名创建队列。
  
  还有一个简单的配置策略可以使得所有的应用放入同一个队列（default），这样就可以让所有应用之间平等共享集群而不是在用户之间。这个配置的定义如下：
  ```
  <queuePlacementPolicy>
  <rule name="default" />
  </queuePlacementPolicy>
  ```
  实现上面功能我们还可以不使用配置文件，直接设置`yarn.scheduler.fair.user-as-default-queue=false`，这样应用便会被放入default 队列，而不是各个用户名队列。另外，我们还可以设置`yarn.scheduler.fair.allow-undeclared-pools=false`，这样用户就无法创建队列了。
  
  
  #### 3.5 抢占
  
  当一个job提交到一个繁忙集群中的空队列时，job并不会马上执行，而是阻塞直到正在运行的job释放系统资源。为了使提交job的执行时间更具预测性（可以设置等待的超时时间），Fair调度器支持抢占。
  
  抢占就是允许调度器杀掉占用超过其应占份额资源队列的containers，这些containers资源便可被分配到应该享有这些份额资源的队列中。需要注意抢占会降低集群的执行效率，因为被终止的containers需要被重新执行。
  
  可以通过设置一个全局的参数yarn.scheduler.fair.preemption=true来启用抢占功能。此外，还有两个参数用来控制抢占的过期时间（这两个参数默认没有配置，需要至少配置一个来允许抢占Container）：
  
  ```
  - minimum share preemption timeout
  - fair share preemption timeout
  ```
  
  如果队列在minimum share preemption timeout指定的时间内未获得最小的资源保障，调度器就会抢占containers。我们可以通过配置文件中的顶级元素`<defaultMinSharePreemptionTimeout>`为所有队列配置这个超时时间；我们还可以在`<queue>`元素内配置`<minSharePreemptionTimeout>`元素来为某个队列指定超时时间。
  
  与之类似，如果队列在fair share preemption timeout指定时间内未获得平等的资源的一半（这个比例可以配置），调度器则会进行抢占containers。这个超时时间可以通过顶级元素`<defaultFairSharePreemptionTimeout>`和元素级元素`<fairSharePreemptionTimeout>`分别配置所有队列和某个队列的超时时间。上面提到的比例可以通过`<defaultFairSharePreemptionThreshold>`(配置所有队列)和`<fairSharePreemptionThreshold>`(配置某个队列)进行配置，默认是0.5。
  
  #### 3.6 配置公平调度模式:
  
  - yarn-site.xml
  ```
  <!--使用　公平调度的策略-->
  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
  </property>
  
  <!--禁止用户创建队列-->
  <property>
    <name>yarn.scheduler.fair.allow-undeclared-pools</name>
    <value>false</value>
  </property>
  
  <!-- 不启用抢占功能　-->
  <property>
    <name>yarn.scheduler.fair.preemption</name>
    <value>false</value>
  </property>
  
  <property>
    <name>yarn.scheduler.fair.allocation.file</name>
    <value>fair-scheduler.xml</value>
  </property>
  
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>32</value>
  </property>
  ```
  
  - fair-scheduler.xml
  
  ```
  <?xml version="1.0"?>
  <allocations>
            <queue name="hadoop">
                  <maxRunningApps>50</maxRunningApps>
                  <weight>4.0</weight>
                  <schedulingPolicy>fair</schedulingPolicy>
                  <aclAdministerApps>*</aclAdministerApps>
                  <aclSubmitApps>*</aclSubmitApps>
            </queue>
            <queue name="default">
                  <maxRunningApps>50</maxRunningApps>
                  <weight>1.0</weight>
                  <schedulingPolicy>fair</schedulingPolicy>
                  <aclAdministerApps>*</aclAdministerApps>
                  <aclSubmitApps>*</aclSubmitApps>
            </queue>
            <queue name="saligia">
                  <maxRunningApps>50</maxRunningApps>
                  <weight>5.0</weight>
                  <schedulingPolicy>fair</schedulingPolicy>
                  <aclAdministerApps>*</aclAdministerApps>
                  <aclSubmitApps>*</aclSubmitApps>
                  <minSharePreemptionTimeout>600</minSharePreemptionTimeout>
                  <fairSharePreemptionTimeout>6000</fairSharePreemptionTimeout>
           </queue>
  </allocations>
  ```
  
  ### 4. HADOOP_PROXY_USER
  
  在Hadoop 1.0之前，用户管理方面是非常脆弱的，在MapReduce中，用户可在任意一台机器上，使用作业属性user.name设置作业的“提交用户”（默认读取linux用户），而JobTracker只凭这个属性值判断作业的拥有者，这极容易让集群处于危险状态。
  
  自从Hadoop 1.0后，Hadoop安全机制出现了，该机制基于成熟的kerberos实现，看起来非常好。但由于Kerberos本身的复杂性，使得Hadoop 启用安全机制后将变得非常难以管理，因此目前极少公司开启该功能。
  
  即使你没有开启Hadoop安全功能，Hadoop默认的安全机制也能够从一定程度上避免旧版本带来的问题。在Hadoop 1.0版本之后，用户作业的提交用户是客户端的作业提交的Linux用户，该用户由Hadoop客户端通过RPC发送给服务器端，用户不能通过参数胡乱修改作业提交者。当然，这样也是存在问题的，如果Hadoop客户端管理混乱，每个人均能够将自己的机器作为客户端，则可以在机器上创建任意linux用户提交作业，这将使得集群处于极大地危险中，因此，Hadoop客户端必须得到受到一定限制。
  
  Hadoop 1.0安全机制的引入，使得Hadoop上层系统不能够将实际用户传递到Hadoop层。举例说明，对于Oozie这个作业调度系统而言，通常我们会创建一个叫“oozie”的用户启动这个服务，这样，所以从Oozie发出的作业的提交者全部变成了oozie，不管原始提交者是user1还是user2，也就是说，实际的用户被掩盖了。
  
  ![](http://dongxicheng.org/wp-content/uploads/2014/03/Hadoop%E7%94%A8%E6%88%B7%E4%BC%AA%E8%A3%85%E6%9C%BA%E5%88%B6.jpg)
  
  为了解决该问题，Hadoop引入了安全伪装的功能，该功能允许一个超级用户代理其他用户执行作业或者命令，但对外看来执行者仍是普通用户，比如在Oozie中，我们需要达到这样的效果：用户user1提交了一个作业流，该作业流的执行者实际上是oozie，但对外看来应是user1（也可以这样理解， 用户user1  sudo 成用户oozie，执行作业流）为此，可以在jobtracker（hadoop 1.0）、主备namenode和resoucemanager（hadoop 2.0）上的core-site.xml中增加以下配置：
  
  ```
  <property>
     <name>hadoop.proxyuser.oozie.groups</name>
     <value>group1,group2<value>
  </property>
  <property>
      <name>hadoop.proxyuser.oozie.hosts</name>
      <value>host1,host2<value>
  </property>
  ```
  
  这里，假设用户user1属于group1（注意，这里的user1和group1都是linux用户和用户组，需要在namenode和jobtracker上进行添加），此外，为了限制客户端随意部署，超级用户代理功能只支持host1和host2两个节点。经过以上配置后，在host1和host2上的客户端上，属于group1和group2的用户可以sudo成oozie用户，执行作业流。
  
  
  ResourceManager 通过　`HADOOP_PROXY_USER` 判断提交者。
'''
tags: []
isStarred: false
isTrashed: false
createdAt: "2017-10-13T13:57:42.403Z"
updatedAt: "2018-11-19T06:43:33.393Z"
